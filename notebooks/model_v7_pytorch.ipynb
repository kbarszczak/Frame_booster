{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad822e88",
   "metadata": {},
   "source": [
    "# Proof of concept notebook for the Frame Booster project\n",
    "- Author: Kamil Barszczak\n",
    "- Contact: kamilbarszczak62@gmail.com\n",
    "- Project: https://github.com/kbarszczak/Frame_booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b91489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9edbc19",
   "metadata": {},
   "source": [
    "#### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b419997",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'D:/Data/Video_Frame_Interpolation/vimeo90k_pytorch'\n",
    "data_subdir = 'data'\n",
    "vis_subdir = 'vis'\n",
    "train_ids = 'train.txt'\n",
    "test_ids = 'test.txt'\n",
    "valid_ids = 'valid.txt'\n",
    "vis_ids = 'vis.txt'\n",
    "\n",
    "width, height = 256, 144\n",
    "epochs = 4\n",
    "batch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10e7d7",
   "metadata": {},
   "source": [
    "#### Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfb90a",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteImageDataset(data.Dataset):\n",
    "    def __init__(self, path, subdir, split_filename, shape):\n",
    "        self.path = path\n",
    "        self.subdir = subdir\n",
    "        self.shape = shape\n",
    "        self.ids = pd.read_csv(os.path.join(path, split_filename), names=[\"ids\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, self.subdir, str(self.ids.iloc[idx, 0]))\n",
    "        imgs = [\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im1')),\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im3'))\n",
    "        ]\n",
    "        true = self._read_bytes_to_tensor(os.path.join(img_path, 'im2'))\n",
    "        return imgs, true\n",
    "    \n",
    "    def _read_bytes_to_tensor(self, path):\n",
    "        with open(path, 'rb') as bf:\n",
    "            return torch.from_numpy(np.transpose(np.reshape(np.frombuffer(bf.read(), dtype='float32'), self.shape), (2, 0, 1)).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = train_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = test_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "valid_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = valid_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "vis_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = vis_subdir,\n",
    "        split_filename = vis_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training batches: {len(train_dataloader)}')\n",
    "print(f'Testing batches: {len(test_dataloader)}')\n",
    "print(f'Validating batches: {len(valid_dataloader)}')\n",
    "print(f'Visualizing batches: {len(vis_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074862e",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        self.__name__ = \"perceptual\"\n",
    "        blocks = []\n",
    "        blocks.append(torchvision.models.vgg16(weights='DEFAULT').features[:4].eval().to(device))\n",
    "        blocks.append(torchvision.models.vgg16(weights='DEFAULT').features[4:9].eval().to(device))\n",
    "        blocks.append(torchvision.models.vgg16(weights='DEFAULT').features[9:16].eval().to(device))\n",
    "        blocks.append(torchvision.models.vgg16(weights='DEFAULT').features[16:23].eval().to(device))\n",
    "        for bl in blocks:\n",
    "            for p in bl.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.blocks = nn.ModuleList(blocks).to(device)\n",
    "        self.transform = F.interpolate\n",
    "        self.resize = resize\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[0, 1, 2, 3]):\n",
    "        input = (input-self.mean) / self.std\n",
    "        target = (target-self.mean) / self.std\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = input\n",
    "        y = target\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            if i in feature_layers:\n",
    "                loss += F.l1_loss(x, y)\n",
    "            if i in style_layers:\n",
    "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
    "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
    "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
    "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return 1 - psnr / 40.0\n",
    "\n",
    "\n",
    "perceptual_loss = VGGPerceptualLoss()\n",
    "    \n",
    "    \n",
    "def loss(y_true, y_pred):\n",
    "    perceptual_loss_ = perceptual_loss(y_true, y_pred)\n",
    "    psnr_ = psnr(y_true, y_pred)\n",
    "    mse_ = mse(y_true, y_pred)\n",
    "    mae_ = mae(y_true, y_pred)\n",
    "    \n",
    "    return 0.5*perceptual_loss_ + psnr_ + 5.0*mae_ + 10.0*mse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TReLU(nn.Module):\n",
    "    def __init__(self, lower=0.0, upper=1.0, **kwargs):\n",
    "        super(TReLU, self).__init__(**kwargs)\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.clip(x, min=self.lower, max=self.upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b14729",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output=False, **kwargs):\n",
    "        super(Conv2dBlock, self).__init__(**kwargs)\n",
    "        self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.act = nn.PReLU() if not output else TReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.cnn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077aad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowEstimator(nn.Module):\n",
    "    def __init__(self, filters, fcount=32, fsize=5, **kwargs):\n",
    "        super(FlowEstimator, self).__init__(**kwargs)\n",
    "        \n",
    "        # ------------- Flow estimation layers\n",
    "        fpadding = fsize // 2\n",
    "        self.cnn_1 = nn.Conv2d(2*filters, fcount, fsize, 1, fpadding)\n",
    "        self.cnn_2 = nn.Conv2d(fcount, fcount, fsize, 1, fpadding)\n",
    "        self.cnn_3 = nn.Conv2d(fcount, fcount, fsize, 1, fpadding)\n",
    "        self.cnn_4 = nn.Conv2d(fcount, fcount, fsize, 1, fpadding)\n",
    "        self.cnn_5 = nn.Conv2d(fcount, fcount, fsize, 1, fpadding)\n",
    "        self.cnn_6 = nn.Conv2d(fcount, 2, fsize, 1, fpadding)\n",
    "\n",
    "    def forward(self, source, target, flow):\n",
    "        if torch.is_tensor(flow):\n",
    "            x0 = FlowPyramid.warp(source, flow)\n",
    "            x0 = torch.cat([x0, target], dim=1)\n",
    "        else:\n",
    "            x0 = torch.cat([source, target], dim=1)\n",
    "        \n",
    "        x1 = self.cnn_1(x0)\n",
    "        x2 = self.cnn_2(x1) + x1\n",
    "        x3 = self.cnn_3(x2) + x2\n",
    "        x4 = self.cnn_4(x3) + x3\n",
    "        x5 = self.cnn_5(x4) + x4\n",
    "        x6 = self.cnn_6(x5)\n",
    "        \n",
    "        if torch.is_tensor(flow):\n",
    "            return flow + x6\n",
    "        else:\n",
    "            return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowPyramid(nn.Module):\n",
    "    def __init__(self, in_channels, filters, fsizes, **kwargs):\n",
    "        super(FlowPyramid, self).__init__(**kwargs)\n",
    "        \n",
    "        # ------------- Unet type encoding layers\n",
    "        self.cnn_1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
    "        self.cnn_2 = nn.Conv2d(in_channels, in_channels, 3, 2, 1)\n",
    "        self.cnn_3 = nn.Conv2d(in_channels, in_channels + filters, 3, 1, 1)\n",
    "        self.cnn_4 = nn.Conv2d(in_channels + filters, in_channels + filters, 3, 2, 1)\n",
    "        self.cnn_5 = nn.Conv2d(in_channels + filters, in_channels + 2*filters, 3, 1, 1)\n",
    "        self.cnn_6 = nn.Conv2d(in_channels + 2*filters, in_channels + 2*filters, 3, 2, 1)\n",
    "        self.cnn_7 = nn.Conv2d(in_channels + 2*filters, in_channels + 3*filters, 3, 1, 1)\n",
    "        \n",
    "        # ------------- Flow estimation layers\n",
    "        self.flow_1 = FlowEstimator(filters=in_channels, fcount=in_channels, fsize=fsizes[0])\n",
    "        self.flow_2 = FlowEstimator(filters=in_channels + filters, fcount=in_channels + filters, fsize=fsizes[1])\n",
    "        self.flow_3 = FlowEstimator(filters=in_channels + 2*filters, fcount=in_channels + 2*filters, fsize=fsizes[2])\n",
    "        self.flow_4 = FlowEstimator(filters=in_channels + 3*filters, fcount=in_channels + 3*filters, fsize=fsizes[3])\n",
    "        \n",
    "        # ------------- Upsample layer\n",
    "        self.upsample = nn.Upsample(scale_factor=(2, 2), mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def _encode(self, x):\n",
    "        out1 = self.cnn_1(x)\n",
    "        out2 = self.cnn_3(self.cnn_2(out1))\n",
    "        out3 = self.cnn_5(self.cnn_4(out2))\n",
    "        out4 = self.cnn_7(self.cnn_6(out3))\n",
    "        return out1, out2, out3, out4\n",
    "    \n",
    "    def _process_flow(self, flow):\n",
    "        return self.upsample(flow) * 2\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # make unet type encoded features\n",
    "        s1, s2, s3, s4 = self._encode(source)\n",
    "        t1, t2, t3, t4 = self._encode(target)\n",
    "        \n",
    "        # calculate flow\n",
    "        f4 = self._process_flow(self.flow_4(s4, t4, None))\n",
    "        f3 = self._process_flow(self.flow_3(s3, t3, f4))\n",
    "        f2 = self._process_flow(self.flow_2(s2, t2, f3))\n",
    "        f1 = self.flow_1(s1, t1, f2)\n",
    "\n",
    "        return f1\n",
    "    \n",
    "    @staticmethod\n",
    "    def warp(image: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = image.size()\n",
    "\n",
    "        xx = torch.arange(0, W).view(1 ,-1).repeat(H, 1)\n",
    "        yy = torch.arange(0, H).view(-1 ,1).repeat(1, W)\n",
    "        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "\n",
    "        grid = torch.cat((xx, yy), 1).float()\n",
    "        if image.is_cuda:\n",
    "            grid = grid.cuda()\n",
    "\n",
    "        vgrid = grid + flow\n",
    "        vgrid[:, 0, :, :] = 2.0 * vgrid[: ,0 ,: ,:].clone() / max(W - 1, 1) - 1.0\n",
    "        vgrid[:, 1, :, :] = 2.0 * vgrid[: ,1 ,: ,:].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "        flow = flow.permute(0, 2, 3, 1)\n",
    "        output = F.grid_sample(image, vgrid, align_corners=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBNet(nn.Module):\n",
    "    def __init__(self, input_shape, device, filters=16, fsizes=[3, 3, 3, 3], **kwargs):\n",
    "        super(FBNet, self).__init__(**kwargs)\n",
    "        \n",
    "        self.b = input_shape[0]\n",
    "        self.c = input_shape[1]\n",
    "        self.h = input_shape[2]\n",
    "        self.w = input_shape[3]\n",
    "        \n",
    "        # ------------- Feature encoding layers\n",
    "        self.cnn_enblock_1 = Conv2dBlock(self.c, filters, 3, 1, 1)\n",
    "        self.cnn_enblock_2 = Conv2dBlock(self.c + filters, filters, 3, 1, 1)\n",
    "        self.cnn_enblock_3 = Conv2dBlock(self.c + 2*filters, filters, 3, 1, 1)\n",
    "        self.cnn_enblock_4 = Conv2dBlock(self.c + 3*filters, filters, 3, 1, 1)\n",
    "        \n",
    "        # ------------- Feature decoding layers\n",
    "        self.cnn_decblock_1 = Conv2dBlock(2*(self.c + 4*filters), 4*filters, 3, 1, 1)\n",
    "        self.cnn_decblock_2 = Conv2dBlock(4*filters, 4*filters, 3, 1, 1)\n",
    "        self.cnn_decblock_3 = Conv2dBlock(4*filters, 2*filters, 3, 1, 1)\n",
    "        self.cnn_decblock_4 = Conv2dBlock(2*filters, 2*filters, 3, 1, 1)\n",
    "        self.cnn_decblock_5 = Conv2dBlock(2*filters, 3, 3, 1, 1, output=True)\n",
    "        \n",
    "        # ------------- Flow pyramid layer\n",
    "        self.flow_pyramid = FlowPyramid(in_channels=self.c + 4*filters, filters=filters, fsizes=fsizes)\n",
    "        \n",
    "    def _encode(self, x):\n",
    "        x = torch.cat([self.cnn_enblock_1(x), x], dim=1)\n",
    "        x = torch.cat([self.cnn_enblock_2(x), x], dim=1)\n",
    "        x = torch.cat([self.cnn_enblock_3(x), x], dim=1)\n",
    "        x = torch.cat([self.cnn_enblock_4(x), x], dim=1)\n",
    "        return x\n",
    "    \n",
    "    def _flow(self, left, right):\n",
    "        flow_left = self.flow_pyramid(left, right)\n",
    "        flow_right = self.flow_pyramid(right, left)\n",
    "        return flow_left, flow_right\n",
    "    \n",
    "    def _decode(self, left, right):\n",
    "        x0 = torch.cat([left, right], dim=1)\n",
    "        x1 = self.cnn_decblock_1(x0)\n",
    "        x2 = self.cnn_decblock_2(x1) + x1\n",
    "        x3 = self.cnn_decblock_3(x2)\n",
    "        x4 = self.cnn_decblock_4(x3) + x3\n",
    "        x5 = self.cnn_decblock_5(x4)\n",
    "        return x5\n",
    "    \n",
    "    def forward(self, left, right):\n",
    "        # encode features\n",
    "        left = self._encode(left)\n",
    "        right = self._encode(right)\n",
    "        \n",
    "        # calculate flow & warp features\n",
    "        flow_left, flow_right = self._flow(left, right)\n",
    "        left = FlowPyramid.warp(left, flow_left)\n",
    "        right = FlowPyramid.warp(right, flow_right)\n",
    "        \n",
    "        # decode features\n",
    "        result = self._decode(left, right)\n",
    "        \n",
    "#        # return the result\n",
    "#         if self.training:\n",
    "#             return result, flow_left\n",
    "#         else:\n",
    "#             return result\n",
    "        # return the result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8749f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fbnet = FBNet(input_shape=(batch, 3, height, width), device=device).to(device)\n",
    "torchsummary.summary(fbnet, [(3, height, width), (3, height, width)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab67d7c",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet(left, right, y, y_pred, figsize=(20, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    data = torch.cat([\n",
    "        torchvision.transforms.functional.rotate(right, 90, expand=True),\n",
    "        torchvision.transforms.functional.rotate(y_pred, 90, expand=True), \n",
    "        torchvision.transforms.functional.rotate(y, 90, expand=True), \n",
    "        torchvision.transforms.functional.rotate(left, 90, expand=True)\n",
    "    ], dim=0)\n",
    "    grid = torchvision.utils.make_grid(data, nrow=left.shape[0])\n",
    "    grid = torchvision.transforms.functional.rotate(grid, 270, expand=True)\n",
    "    plt.imshow(torch.permute(grid, (1, 2, 0)).cpu())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train, valid, optimizer, loss, metrics, epochs, batch, save_freq=500, log_freq=1, log_perf_freq=2500, mode=\"best\"):  \n",
    "    # create dict for a history\n",
    "    history = {loss.__name__: []} | {metric.__name__: [] for metric in metrics} | {'val_' + loss.__name__: []} | {\"val_\" + metric.__name__: [] for metric in metrics}\n",
    "    best_loss = None\n",
    "    \n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # create empty dict for loss and metrics\n",
    "        loss_metrics = {loss.__name__: []} | {metric.__name__: [] for metric in metrics}\n",
    "\n",
    "        # loop over training batches\n",
    "        model.train(True)\n",
    "        for step, record in enumerate(train):\n",
    "            start = time.time()\n",
    "            \n",
    "            # extract the data\n",
    "            left, right, y = record[0][0].to(device), record[0][1].to(device), record[1].to(device)\n",
    "\n",
    "            # clear gradient\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            y_pred = model(left, right) \n",
    "            \n",
    "            # calculate loss and apply the gradient\n",
    "            loss_value = loss(y, y_pred)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate metrics\n",
    "            y_pred_detached = y_pred.detach()\n",
    "            metrics_values = [metric(y, y_pred_detached) for metric in metrics]\n",
    "            \n",
    "            # save the loss and metrics\n",
    "            loss_metrics[loss.__name__].append(loss_value.item())\n",
    "            for metric, value in zip(metrics, metrics_values):\n",
    "                loss_metrics[metric.__name__].append(value.item())\n",
    "                \n",
    "            end = time.time()\n",
    "            \n",
    "            # save the model\n",
    "            if save_freq is not None and step % save_freq == 0 and step > 0:\n",
    "                loss_avg = np.mean(loss_metrics[loss.__name__])\n",
    "                if mode == \"all\" or (mode == \"best\" and (best_loss is None or best_loss > loss_avg)):\n",
    "                    filename = f'../models/model_v6_1/fbnet_l={loss_avg}_e={epoch+1}_t={int(time.time())}.pth'\n",
    "                    torch.save(model.state_dict(), filename)\n",
    "                    \n",
    "            # log the model performance\n",
    "            if log_perf_freq is not None and step % log_perf_freq == 0 and step > 0:\n",
    "                plot_triplet(left, right, y, y_pred.detach())\n",
    "                \n",
    "            # log the state\n",
    "            if step % log_freq == 0:\n",
    "                time_left = (end-start) * (len(train) - (step+1))\n",
    "                print('\\r[%5d/%5d] (eta: %s)' % ((step+1), len(train), time.strftime('%H:%M:%S', time.gmtime(time_left))), end='')\n",
    "                for metric, values in loss_metrics.items():\n",
    "                    print(f' {metric}=%.4f' % (np.mean(values)), end='')\n",
    "            \n",
    "        # save the training history\n",
    "        for metric, values in loss_metrics.items():\n",
    "            history[metric].extend(values)\n",
    "\n",
    "        # setup dict for validation loss and metrics\n",
    "        loss_metrics = {loss.__name__: []} | {metric.__name__: [] for metric in metrics}\n",
    "        \n",
    "        # process the full validating dataset\n",
    "        model.train(False)\n",
    "        for step, record in enumerate(valid):\n",
    "            left, right, y = record[0][0].to(device), record[0][1].to(device), record[1].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(left, right).detach()\n",
    "            \n",
    "            # save the loss and metrics\n",
    "            loss_metrics[loss.__name__].append(loss(y, y_pred).item())\n",
    "            for metric, value in zip(metrics, [metric(y, y_pred) for metric in metrics]):\n",
    "                loss_metrics[metric.__name__].append(value.item())\n",
    "            \n",
    "        # log the validation score & save the validation history\n",
    "        for metric, values in loss_metrics.items():\n",
    "            print(f' val_{metric}=%.4f' % (np.mean(values)), end='')\n",
    "            history[f\"val_{metric}\"].extend(values)\n",
    "            \n",
    "        # restart state printer\n",
    "        print()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bd825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = fit(\n",
    "    model = fbnet, \n",
    "    train = train_dataloader,\n",
    "    valid = valid_dataloader,\n",
    "    optimizer = optim.NAdam(fbnet.parameters(), lr=1e-4), \n",
    "    loss = loss, \n",
    "    metrics = [psnr],\n",
    "    epochs = epochs, \n",
    "    batch = batch, \n",
    "    save_freq = 500,\n",
    "    log_freq = 1,\n",
    "    log_perf_freq = 2500,\n",
    "    mode = \"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fbnet.state_dict(), f'../models/model_v6_1/fbnet_e={epochs}_t={int(time.time())}.pth')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022c2c2",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_0_1(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def compress_loss(loss, steps):\n",
    "    return [np.average(loss[steps*i:steps*(i+1)]) for i in range(len(loss)//steps)]\n",
    "\n",
    "def plot_history(history, norm=norm_0_1, figsize=(10,5), steps=None):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    metrics = list(history.keys())\n",
    "    metrics = [metric for metric in metrics if \"val\" not in metric]\n",
    "    \n",
    "    if steps is None:\n",
    "        data = [(index, history[metric], history['val_'+metric], metric) for index, metric in enumerate(metrics)]\n",
    "    else:\n",
    "        tlen = len(history[metrics[0]])\n",
    "        vlen = len(history['val_'+metrics[0]])\n",
    "        data = [(index, compress_loss(history[metric], tlen//steps), compress_loss(history['val_'+metric], vlen//steps), metric) for index, metric in enumerate(metrics)]\n",
    "        \n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    epochs = range(1, len(data[0][1]) + 1)\n",
    "    \n",
    "    for index, value, val_value, metric in data:\n",
    "        if norm is not None:\n",
    "            buffer = norm(value + val_value)\n",
    "            value = buffer[0:len(epochs)]\n",
    "            val_value = buffer[len(epochs):]\n",
    "        \n",
    "        plt.plot(epochs, value, colors[index], label=f\"train {metric}\")\n",
    "        plt.plot(epochs, val_value, colors[index]+'--', label=f\"valid {metric}\")\n",
    "        \n",
    "    plt.title(\"Comparision of training and validating scores\")\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel(\"Values\" if norm is None else \"Values normalized\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0607e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, loss, metrics):\n",
    "    loss_metrics = {loss.__name__: []} | {metric.__name__: [] for metric in metrics}\n",
    "    model.train(False)\n",
    "    for step, record in enumerate(data):\n",
    "        left, right, y = record[0][0].to(device), record[0][1].to(device), record[1].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(left, right).detach()\n",
    "\n",
    "        # save the loss and metrics\n",
    "        loss_metrics[loss.__name__].append(loss(y, y_pred).item())\n",
    "        for metric, value in zip(metrics, [metric(y, y_pred) for metric in metrics]):\n",
    "            loss_metrics[metric.__name__].append(value.item())\n",
    "            \n",
    "        print('\\rProgress [%4d/%4d]' % ((step+1), len(data)), end='')\n",
    "        \n",
    "    print()\n",
    "            \n",
    "    return {k: np.mean(v) for k, v in loss_metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, norm=None, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d68f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(fbnet, test_dataloader, loss, [mae, mse, psnr])\n",
    "for k, v in results.items():\n",
    "    print('%s: %.6f' % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36dabf",
   "metadata": {},
   "source": [
    "#### Visualize net output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ab9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_grid(data, nrow, figsize):\n",
    "    if figsize == 'auto':\n",
    "        figsize = (20, 5*(data.shape[0]//nrow))\n",
    "    \n",
    "    if data is not None:\n",
    "        plt.figure(figsize=figsize)\n",
    "        grid = torchvision.utils.make_grid(data, nrow=nrow)\n",
    "        plt.imshow(torch.permute(grid, (1, 2, 0)).cpu())\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783748ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_output(model, batches, figsize='auto'):\n",
    "    data = None\n",
    "    batch_size = batches[0][1].shape[0]\n",
    "    model.train(False)\n",
    "    for batch in batches:\n",
    "        left, right, y = batch[0][0].to(device), batch[0][1].to(device), batch[1].to(device)\n",
    "        y_pred = model(left, right).detach()\n",
    "        for index in range(batch_size):\n",
    "            cat_list = [\n",
    "                torch.unsqueeze(left[index, :, :, :], 0),\n",
    "                torch.unsqueeze(y[index, :, :, :], 0),\n",
    "                torch.unsqueeze(y_pred[index, :, :, :], 0),\n",
    "                torch.unsqueeze(right[index, :, :, :], 0)\n",
    "            ]\n",
    "            if data is not None:\n",
    "                cat_list = [data] + cat_list\n",
    "            data = torch.cat(cat_list, dim=0)\n",
    "                \n",
    "    display_grid(data, 4, figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_iterator = iter(vis_dataloader)\n",
    "visualise_output(fbnet, batches=[next(vis_iterator) for bi in range(len(vis_dataloader)) if bi in [0, 1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a14ee5",
   "metadata": {},
   "source": [
    "#### Visualize conv2d filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fcb9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def vis_conv2d_weight(kernels, all_kernels=True, nrow=32, padding=1, ch=0): \n",
    "    b, c, w, h = kernels.shape\n",
    "\n",
    "    if all_kernels: \n",
    "        kernels = kernels.view(b*c, -1, w, h)\n",
    "    elif c != 3: \n",
    "        kernels = kernels[:, ch, :, :].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((kernels.shape[0] // nrow + 1, 64))    \n",
    "    grid = torchvision.utils.make_grid(kernels, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure(figsize=(nrow, rows))\n",
    "    plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def deprocess_image(img):\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    \n",
    "    return img\n",
    "    \n",
    "    \n",
    "def append_image(image, append_image, row, col, margin):\n",
    "    horizontal_start = row * height + row * margin\n",
    "    horizontal_end = horizontal_start + height\n",
    "    vertical_start = col * width + col * margin\n",
    "    vertical_end = vertical_start + width\n",
    "    image[horizontal_start : horizontal_end, vertical_start : vertical_end, : ] = append_image\n",
    "    return image\n",
    "\n",
    "    \n",
    "def rows_cols(value):\n",
    "    assert value >= 1\n",
    "    rows, cols = 1, value\n",
    "    for i in range(2, value//2+1):\n",
    "        if value % i == 0:\n",
    "            if np.abs(i - int(value / i)) < np.abs(rows - cols):\n",
    "                rows = i\n",
    "                cols = int(value / i)\n",
    "    return rows, cols\n",
    "    \n",
    "\n",
    "def visualize_cnn_layers(model, margin=3, steps=10, lr=0.1, include_nested=True):\n",
    "    assert margin >= 0, \"Margin cannot be negative\"\n",
    "    assert steps > 0, \"Steps has to be positive\"\n",
    "    \n",
    "    activations = {}\n",
    "    def hook_fun(model, input, output):\n",
    "        activations['activation'] = output\n",
    "    \n",
    "    model.train(False)\n",
    "    queue = list(model.named_children())\n",
    "    while queue:\n",
    "        name, layer = queue.pop(0)\n",
    "        if include_nested:\n",
    "            queue.extend([(f'{name}_{n}', l) for n, l in layer.named_children()])\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n",
    "            print(f\"Layer name: {name}\")\n",
    "            \n",
    "            f_count = layer.out_channels\n",
    "            rows, cols = rows_cols(f_count)\n",
    "            result = np.zeros((rows * height + (rows-1) * margin, cols * width + (cols-1) * margin, 3), dtype='uint8')\n",
    "\n",
    "            for index in tqdm.tqdm(range(rows*cols)):\n",
    "                i, j = index//cols, index % cols\n",
    "                filter_index = j + (i * cols)\n",
    "                \n",
    "                hook = layer.register_forward_hook(hook_fun)\n",
    "                noise = (np.random.rand(batch, model.c, model.h, model.w) * 0.2 + 0.4).astype('float32')\n",
    "                tensor = torch.from_numpy(noise).to(device).requires_grad_(True)\n",
    "                optimizer = optim.NAdam([tensor], lr=lr)\n",
    "                \n",
    "                for step in range(steps):\n",
    "                    optimizer.zero_grad()\n",
    "                    _ = model(tensor, tensor)\n",
    "                    activation = activations['activation'][:, filter_index, :, :].unsqueeze(dim=1)\n",
    "                    loss = torch.mean(activation)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                tensor = tensor.detach()[0].permute((1, 2, 0))\n",
    "                filter_img = deprocess_image(tensor.cpu().numpy())\n",
    "                result = append_image(result, filter_img, i, j, margin)\n",
    "                hook.remove()\n",
    "\n",
    "            plt.figure(figsize=(model.h // 2, model.w // 2))\n",
    "            plt.imshow(result)\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8d7f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_cnn_layers(fbnet, margin=3, steps=10, lr=0.1, include_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_conv2d_weight(fbnet.cnn_r1_1.weight.detach(), nrow=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dc204",
   "metadata": {},
   "source": [
    "#### Visualize inner optical flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5007cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_flow(model, batches, figsize='auto'):\n",
    "    flows, names, hooks = {}, [], []\n",
    "    \n",
    "    # hook registration function\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            flows[name] = output[-1].detach()\n",
    "        return hook\n",
    "\n",
    "    # register hooks\n",
    "    for child in fbnet.named_children():\n",
    "        if \"flow\" in child[0]:\n",
    "            hooks.append(child[1].register_forward_hook(get_activation(child[0])))\n",
    "            names.append(child[0])\n",
    "            \n",
    "    # iterate over the given batches\n",
    "    model.train(False)\n",
    "    data, batch_size = None, batches[0][1].shape[0]\n",
    "    resize = torchvision.transforms.Resize((height, width), antialias=True)\n",
    "    for batch in batches:\n",
    "        # forward pass (hooks register outputs)\n",
    "        left, right = batch[0][0].to(device), batch[0][1].to(device)\n",
    "        _ = model(left, right)\n",
    "        \n",
    "        # process each sample in the batch\n",
    "        for index in range(batch_size):\n",
    "            cat_list = [torch.unsqueeze(left[index, :, :, :], dim=0)]\n",
    "            \n",
    "            for name in names:\n",
    "                flow = flows[name][index, :, :, :]\n",
    "                flow = resize(torchvision.utils.flow_to_image(flow)) / 255.0\n",
    "                cat_list.append(torch.unsqueeze(flow, dim=0))\n",
    "                \n",
    "            cat_list.append(torch.unsqueeze(right[index, :, :, :], dim=0))\n",
    "            \n",
    "            if data is not None:\n",
    "                cat_list = [data] + cat_list\n",
    "                \n",
    "            data = torch.cat(cat_list, dim=0)\n",
    "\n",
    "    # display data\n",
    "    display_grid(data, (2 + len(names)), figsize)\n",
    "        \n",
    "    # remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f0524",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_iterator = iter(vis_dataloader)\n",
    "visualise_flow(fbnet, batches=[next(vis_iterator) for bi in range(len(vis_dataloader)) if bi in [0, 1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb698b3",
   "metadata": {},
   "source": [
    "#### Visualise attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_attention(model, batches, figsize='auto'):\n",
    "    attentions, names, hooks = {}, [], []\n",
    "    \n",
    "    # hook registration function\n",
    "    def get_activation(name, act, upsample):\n",
    "        def hook(model, input, output):\n",
    "            attentions[name] = upsample(act(output.detach()))\n",
    "        return hook\n",
    "\n",
    "    # register hooks\n",
    "    for child in model.named_children():\n",
    "        if isinstance(child[1], AttentionGate):\n",
    "            hooks.append(child[1].ocnn.register_forward_hook(get_activation(child[0], child[1].out_act, child[1].upsample)))\n",
    "            names.append(child[0])\n",
    "            \n",
    "    # iterate over the given batches\n",
    "    model.train(False)\n",
    "    data, batch_size = None, batches[0][1].shape[0]\n",
    "    resize = torchvision.transforms.Resize((height, width), antialias=True)\n",
    "    gray2rgb = torchvision.transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "    for batch in batches:\n",
    "        # forward pass (hooks register outputs)\n",
    "        left, right = batch[0][0].to(device), batch[0][1].to(device)\n",
    "        _ = model(left, right)\n",
    "        \n",
    "        # process each sample in the batch\n",
    "        for index in range(batch_size):\n",
    "            cat_list = [torch.unsqueeze(left[index, :, :, :], dim=0)]\n",
    "            \n",
    "            for name in names:\n",
    "                attention = attentions[name][index, :, :, :]\n",
    "                attention = gray2rgb(resize(attention))\n",
    "                cat_list.append(torch.unsqueeze(attention, dim=0))\n",
    "                \n",
    "            cat_list.append(torch.unsqueeze(right[index, :, :, :], dim=0))\n",
    "            \n",
    "            if data is not None:\n",
    "                cat_list = [data] + cat_list\n",
    "                \n",
    "            data = torch.cat(cat_list, dim=0)\n",
    "\n",
    "    # display data\n",
    "    display_grid(data, (2 + len(names)), figsize)\n",
    "        \n",
    "    # remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_iterator = iter(vis_dataloader)\n",
    "visualise_attention(fbnet, batches=[next(vis_iterator) for bi in range(len(vis_dataloader)) if bi in [0, 1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf098604",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbnet.load_state_dict(torch.load('../tmp/model_v6_3/1686927783/models/fbnet_l=1.4013603990221608_e=1_s=12001_t=1686933041.pt'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f29089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
