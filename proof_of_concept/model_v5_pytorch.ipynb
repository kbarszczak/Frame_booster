{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d37e4af",
   "metadata": {},
   "source": [
    "# Proof of concept notebook for the Frame Booster project\n",
    "- Author: Kamil Barszczak\n",
    "- Contact: kamilbarszczak62@gmail.com\n",
    "- Project: https://github.com/kbarszczak/Frame_booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "049c8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchsummary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a20cf",
   "metadata": {},
   "source": [
    "#### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dd4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'E:/Data/Video_Frame_Interpolation/processed/vimeo90k_pytorch'\n",
    "data_subdir = 'data'\n",
    "train_ids = 'train.txt'\n",
    "test_ids = 'test.txt'\n",
    "valid_ids = 'valid.txt'\n",
    "\n",
    "width, height = 256, 144\n",
    "epochs = 5\n",
    "batch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510896a",
   "metadata": {},
   "source": [
    "#### Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b710df",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e23b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteImageDataset(data.Dataset):\n",
    "    def __init__(self, path, subdir, split_filename, shape):\n",
    "        self.path = path\n",
    "        self.subdir = subdir\n",
    "        self.shape = shape\n",
    "        self.ids = pd.read_csv(os.path.join(path, split_filename), names=[\"ids\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, self.subdir, str(self.ids.iloc[idx, 0]))\n",
    "        \n",
    "        imgs = [\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im1')),\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im3'))\n",
    "        ]\n",
    "        true = self._read_bytes_to_tensor(os.path.join(img_path, 'im2'))\n",
    "        \n",
    "        return imgs, true\n",
    "    \n",
    "    def _read_bytes_to_tensor(self, path):\n",
    "        with open(path, 'rb') as bf:\n",
    "            return torch.permute(torch.reshape(torch.frombuffer(bf.read(), dtype=torch.float), self.shape), (2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c64b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = train_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = test_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "valid_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = valid_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89753871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 10000\n",
      "Testing batches: 600\n",
      "Validating batches: 200\n"
     ]
    }
   ],
   "source": [
    "print(f'Training batches: {len(train_dataloader)}')\n",
    "print(f'Testing batches: {len(test_dataloader)}')\n",
    "print(f'Validating batches: {len(valid_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed31bc",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bd7ad5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncateActivation(nn.Module):\n",
    "    def __init__(self, lower=0.0, upper=1.0, **kwargs):\n",
    "        super(TruncateActivation, self).__init__(**kwargs)\n",
    "        self.a = lower\n",
    "        self.b = upper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.min(torch.max(x, self.a), self.b)\n",
    "\n",
    "    \n",
    "def l1(y_true, y_pred):\n",
    "    return torch.sum(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def l2(y_true, y_pred):\n",
    "    return torch.sqrt(torch.sum((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return 1 - psnr / 40.0\n",
    "\n",
    "\n",
    "# def ssim(y_true, y_pred):\n",
    "#     ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "#     return 1 - ssim\n",
    "\n",
    "    \n",
    "def loss(y_true, y_pred):\n",
    "    # ssim_ = ssim(y_true, y_pred)\n",
    "    psnr_ = psnr(y_true, y_pred)\n",
    "    mse_ = mse(y_true, y_pred)\n",
    "    mae_ = mae(y_true, y_pred)\n",
    "    \n",
    "    # return ssim_ + psnr_ + 5.0*l1_ + 10.0*l2_    \n",
    "    return psnr_ + 5.0*mae_ + 10.0*mse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0977db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowEstimation(nn.Module):\n",
    "    def __init__(self, flow_input_chanels,\n",
    "                flow_info = {\n",
    "                    \"filter_counts\": [32, 64, 64, 16, 12, 12],\n",
    "                    \"filter_sizes\": [(3, 3), (3, 3), (3, 3), (3, 3), (1, 1), (1, 1)],\n",
    "                    \"filter_strides\": [1, 1, 1, 1, 1, 1],\n",
    "                    \"filter_padding\": [1, 1, 1, 1, 0, 0],\n",
    "                    \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()],\n",
    "                }, **kwargs):\n",
    "        super(FlowEstimation, self).__init__(**kwargs)\n",
    "\n",
    "        modules = []\n",
    "        last_output_size = flow_input_chanels\n",
    "        for fcount, fsize, fstride, fpad, fact in zip(flow_info['filter_counts'], flow_info['filter_sizes'], flow_info['filter_strides'], flow_info['filter_padding'], flow_info['activations']):\n",
    "            modules.append(nn.Conv2d(last_output_size, fcount, fsize, fstride, fpad))\n",
    "            modules.append(fact)\n",
    "            last_output_size = fcount\n",
    "\n",
    "        modules.append(nn.Conv2d(last_output_size, 2, 1))\n",
    "        self.flow = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "7c221682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalFeatureWarp(nn.Module):\n",
    "    def __init__(self, flow_prediction, interpolation='bilinear', **kwargs):\n",
    "        super(BidirectionalFeatureWarp, self).__init__(**kwargs)\n",
    "        \n",
    "        self.flow_prediction = flow_prediction\n",
    "        self.flow_upsample_1_2 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        self.flow_upsample_2_1 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "\n",
    "    def forward(self, input_1, input_2, flow_1_2, flow_2_1):\n",
    "        if torch.is_tensor(flow_1_2) and torch.is_tensor(flow_2_1):\n",
    "            input_1_warped_1 = BidirectionalFlowEstimation.warp(input_1, flow_1_2)\n",
    "            input_2_warped_1 = BidirectionalFlowEstimation.warp(input_2, flow_2_1)\n",
    "        else:\n",
    "            input_1_warped_1 = input_1\n",
    "            input_2_warped_1 = input_2\n",
    "            \n",
    "        flow_change_1_2_concat = torch.cat([input_2, input_1_warped_1], dim=1)\n",
    "        flow_change_1_2 = self.flow_prediction(flow_change_1_2_concat)\n",
    "        \n",
    "        flow_change_2_1_concat = torch.cat([input_1, input_2_warped_1], dim=1)\n",
    "        flow_change_2_1 = self.flow_prediction(flow_change_2_1_concat)\n",
    "        \n",
    "        if torch.is_tensor(flow_1_2) and torch.is_tensor(flow_2_1):\n",
    "            flow_1_2_changed = flow_1_2 + flow_change_1_2\n",
    "            flow_2_1_changed = flow_2_1 + flow_change_2_1\n",
    "        else:\n",
    "            flow_1_2_changed = flow_change_1_2\n",
    "            flow_2_1_changed = flow_change_2_1\n",
    "            \n",
    "        input_1_warped_2 = BidirectionalFlowEstimation.warp(input_1, flow_1_2_changed)\n",
    "        input_2_warped_2 = BidirectionalFlowEstimation.warp(input_2, flow_2_1_changed)\n",
    "        flow_1_2_changed_upsampled = self.flow_upsample_1_2(flow_1_2_changed)\n",
    "        flow_2_1_changed_upsampled = self.flow_upsample_2_1(flow_2_1_changed)\n",
    "        \n",
    "        return input_1_warped_2, input_2_warped_2, flow_1_2_changed_upsampled, flow_2_1_changed_upsampled\n",
    "    \n",
    "    @staticmethod\n",
    "    def warp(image: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = image.size()\n",
    "\n",
    "        xx = torch.arange(0, W).view(1 ,-1).repeat(H, 1)\n",
    "        yy = torch.arange(0, H).view(-1 ,1).repeat(1, W)\n",
    "        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "\n",
    "        grid = torch.cat((xx, yy), 1).float()\n",
    "        if image.is_cuda:\n",
    "            grid = grid.cuda()\n",
    "\n",
    "        vgrid = grid + flow\n",
    "        vgrid[:, 0, :, :] = 2.0 * vgrid[: ,0 ,: ,:].clone() / max(W - 1, 1) - 1.0\n",
    "        vgrid[:, 1, :, :] = 2.0 * vgrid[: ,1 ,: ,:].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "        flow = flow.permute(0, 2, 3, 1)\n",
    "        output = F.grid_sample(image, vgrid)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 encoder_filters = [\n",
    "                     [64, 48, 32, 32],  # encoder_filters_col_1\n",
    "                     [48, 32, 32],  # encoder_filters_col_2\n",
    "                     [32, 32]  # encoder_filters_col_3\n",
    "                 ], \n",
    "                 decoder_filters = [32, 24],  # decoder_filters\n",
    "                 flow_info = [\n",
    "                     {  # flow_1\n",
    "                        \"filter_counts\": [32, 48, 64, 80, 80, 48],\n",
    "                        \"filter_sizes\": [7, 5, 5, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1, 1, 1],\n",
    "                        \"filter_padding\": [3, 2, 2, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     }, \n",
    "                     {  # flow_2\n",
    "                        \"filter_counts\": [24, 32, 64, 64, 32],\n",
    "                        \"filter_sizes\": [5, 3, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1, 1],\n",
    "                        \"filter_padding\": [2, 1, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     }, \n",
    "                     {  # flow_3\n",
    "                        \"filter_counts\": [24, 48, 48, 16],\n",
    "                        \"filter_sizes\": [3, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1],\n",
    "                        \"filter_padding\": [1, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     },\n",
    "                 ], interpolation=\"bilinear\", **kwargs):\n",
    "        super(FBNet, self).__init__(**kwargs)\n",
    "        \n",
    "        self.c = input_shape[0]\n",
    "        self.h = input_shape[1]\n",
    "        self.w = input_shape[2]\n",
    "        \n",
    "        # ------------- Shared Conv2d, AvgPool2d & Resize encoding layers\n",
    "        self.resize_1_2 = torchvision.transforms.Resize(size=(height//2, width//2))\n",
    "        self.resize_1_4 = torchvision.transforms.Resize(size=(height//4, width//4))\n",
    "        self.resize_1_8 = torchvision.transforms.Resize(size=(height//8, width//8))\n",
    "        \n",
    "        self.cnn_r1_c1 = nn.Conv2d(self.c, encoder_filters[0][0], 3, 1, 1, name=\"fe_conv2d_r1_c1\")\n",
    "        self.cnn_r2_c1 = nn.Conv2d(self.c, encoder_filters[0][1], 3, 1, 1, name=\"fe_conv2d_r2_c1\")\n",
    "        self.cnn_r3_c1 = nn.Conv2d(self.c, encoder_filters[0][2], 3, 1, 1, name=\"fe_conv2d_r3_c1\")\n",
    "        self.cnn_r4_c1 = nn.Conv2d(self.c, encoder_filters[0][3], 3, 1, 1, name=\"fe_conv2d_r4_c1\")\n",
    "        selc.act_r1_c1 = nn.PReLU()\n",
    "        selc.act_r2_c1 = nn.PReLU()\n",
    "        selc.act_r3_c1 = nn.PReLU()\n",
    "        selc.act_r4_c1 = nn.PReLU()\n",
    "\n",
    "        self.cnn_r2_c2 = nn.Conv2d(encoder_filters[0][0], encoder_filters[1][0], 3, 1, 1, name=\"fe_conv2d_r2_c2\")\n",
    "        self.cnn_r3_c2 = nn.Conv2d(encoder_filters[0][1], encoder_filters[1][1], 3, 1, 1, name=\"fe_conv2d_r3_c2\")\n",
    "        self.cnn_r4_c2 = nn.Conv2d(encoder_filters[0][2], encoder_filters[1][2], 3, 1, 1, name=\"fe_conv2d_r4_c2\")\n",
    "        selc.act_r2_c2 = nn.PReLU()\n",
    "        selc.act_r3_c2 = nn.PReLU()\n",
    "        selc.act_r4_c2 = nn.PReLU()\n",
    "\n",
    "        self.cnn_r3_c3 = nn.Conv2d(encoder_filters[1][0], encoder_filters[2][0], 3, 1, 1, name=\"fe_conv2d_r3_c3\")\n",
    "        self.cnn_r4_c3 = nn.Conv2d(encoder_filters[1][1], encoder_filters[2][1], 3, 1, 1, name=\"fe_conv2d_r4_c3\")\n",
    "        selc.act_r3_c3 = nn.PReLU()\n",
    "        selc.act_r4_c3 = nn.PReLU()\n",
    "        \n",
    "        self.avg_r2_c1 = nn.AvgPool2d(2, name=\"avg_r2_c1\")\n",
    "        self.avg_r3_c1 = nn.AvgPool2d(2, name=\"avg_r3_c1\")\n",
    "        self.avg_r4_c1 = nn.AvgPool2d(2, name=\"avg_r4_c1\")\n",
    "        \n",
    "        self.avg_r3_c2 = nn.AvgPool2d(2, name=\"avg_r3_c2\")\n",
    "        self.avg_r4_c2 = nn.AvgPool2d(2, name=\"avg_r3_c2\")\n",
    "        \n",
    "        # ------------- Feature warping layers \n",
    "        self.bidirectional_warp_row_1 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = encoder_filters[0][0],\n",
    "                flow_info = flow_info[0]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        self.bidirectional_warp_row_2 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = encoder_filters[0][1] + encoder_filters[1][0],\n",
    "                flow_info = flow_info[1]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        self.bidirectional_warp_row_3 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0],\n",
    "                flow_info = flow_info[2]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        \n",
    "        # ------------- Decoding Conv2d layers\n",
    "        self.cnn_r4_1 = nn.Conv2d(encoder_filters[0][3] + encoder_filters[1][2] + encoder_filters[2][1], encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0], 3, 1, 1, name=\"fus_conv2d_r_4_1\")\n",
    "        self.act_r4_1 = nn.PReLU()\n",
    "        self.up_r4 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r3_1 = nn.Conv2d(encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0], encoder_filters[0][1] + encoder_filters[1][0], 3, 1, 1, name=\"fus_conv2d_r_3_1\")\n",
    "        self.act_r3_1 = nn.PReLU()\n",
    "        self.up_r3 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r2_1 = nn.Conv2d(encoder_filters[0][1] + encoder_filters[1][0], encoder_filters[0][0], 3, 1, 1, name=\"fus_conv2d_r_2_1\")\n",
    "        self.cnn_r2_2 = nn.Conv2d(encoder_filters[0][0], encoder_filters[0][0], 3, 1, 1, name=\"fus_conv2d_r_2_2\")\n",
    "        self.act_r2_1 = nn.PReLU()\n",
    "        self.act_r2_2 = nn.PReLU()\n",
    "        self.up_r2 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r1_1 = nn.Conv2d(encoder_filters[0][0], decoder_filters[0], 3, 1, 1, name=\"fus_conv2d_r_1_1\")\n",
    "        self.cnn_r1_2 = nn.Conv2d(decoder_filters[0], decoder_filters[1], 3, 1, 1, name=\"fus_conv2d_r_1_2\")\n",
    "        self.act_r1_1 = nn.PReLU()\n",
    "        self.act_r1_2 = nn.PReLU()\n",
    "        \n",
    "        self.cnn_out = nn.Conv2d(decoder_filters[1], 3, 1, 1, 0, name=\"fus_conv2d_output\")\n",
    "        self.act_out = TruncateActivation()\n",
    "\n",
    "    def forward(self, input_1_left, input_1_right):\n",
    "        # ------------- Process left input\n",
    "        input_2_left = self.resize_1_2(input_1_left)\n",
    "        input_3_left = self.resize_1_4(input_2_left)\n",
    "        input_4_left = self.resize_1_8(input_3_left)\n",
    "        \n",
    "        # Feature extraction for layer 1\n",
    "        input_1_left_cnn_r1_c1 = self.act_r1_c1(self.cnn_r1_c1(input_1_left))\n",
    "        input_2_left_cnn_r2_c1 = self.act_r2_c1(self.cnn_r2_c1(input_2_left))\n",
    "        input_3_left_cnn_r3_c1 = self.act_r3_c1(self.cnn_r3_c1(input_3_left))\n",
    "        input_4_left_cnn_r4_c1 = self.act_r4_c1(self.cnn_r4_c1(input_4_left))\n",
    "\n",
    "        # Downsample layer 1\n",
    "        input_1_left_cnn_r2_c1 = self.avg_r2_c1(input_1_left_cnn_r1_c1)\n",
    "        input_2_left_cnn_r3_c1 = self.avg_r3_c1(input_2_left_cnn_r2_c1)\n",
    "        input_3_left_cnn_r4_c1 = self.avg_r4_c1(input_3_left_cnn_r3_c1)\n",
    "\n",
    "        # Feature extraction for layer 2\n",
    "        input_1_left_cnn_r2_c2 = self.act_r2_c2(self.cnn_r2_c2(input_1_left_cnn_r2_c1))\n",
    "        input_2_left_cnn_r3_c2 = self.act_r3_c2(self.cnn_r3_c2(input_2_left_cnn_r3_c1))\n",
    "        input_3_left_cnn_r4_c2 = self.act_r4_c2(self.cnn_r4_c2(input_3_left_cnn_r4_c1))\n",
    "\n",
    "        # Downsample layer 2\n",
    "        input_1_left_cnn_r3_c2 = self.avg_r3_c2(input_1_left_cnn_r2_c2)\n",
    "        input_2_left_cnn_r4_c2 = self.avg_r4_c2(input_2_left_cnn_r3_c2)\n",
    "\n",
    "        # Feature extraction for layer 3\n",
    "        input_1_left_cnn_r3_c3 = self.act_r3_c3(self.cnn_r3_c3(input_1_left_cnn_r3_c2))\n",
    "        input_2_left_cnn_r4_c3 = self.act_r4_c3(self.cnn_r4_c3(input_2_left_cnn_r4_c2))\n",
    "\n",
    "        # Concatenate\n",
    "        concat_left_row_2 = torch.cat([input_2_left_cnn_r2_c1, input_1_left_cnn_r2_c2], dim=1)\n",
    "        concat_left_row_3 = torch.cat([input_3_left_cnn_r3_c1, input_2_left_cnn_r3_c2, input_1_left_cnn_r3_c3], dim=1)\n",
    "        concat_left_row_4 = torch.cat([input_4_left_cnn_r4_c1, input_3_left_cnn_r4_c2, input_2_left_cnn_r4_c3], dim=1)\n",
    "        \n",
    "        # Feature extraction left side output: \n",
    "        # * input_1_left_cnn_r1_c1\n",
    "        # * concat_left_row_2\n",
    "        # * concat_left_row_3\n",
    "        # * concat_left_row_4\n",
    "        \n",
    "        # ------------- Process right input\n",
    "        input_2_right = self.resize_1_2(input_1_right)\n",
    "        input_3_right = self.resize_1_4(input_2_right)\n",
    "        input_4_right = self.resize_1_8(input_3_right)\n",
    "\n",
    "        # Feature extraction for layer 1\n",
    "        input_1_right_cnn_r1_c1 = self.act_r1_c1(self.cnn_r1_c1(input_1_right))\n",
    "        input_2_right_cnn_r2_c1 = self.act_r2_c1(self.cnn_r2_c1(input_2_right))\n",
    "        input_3_right_cnn_r3_c1 = self.act_r3_c1(self.cnn_r3_c1(input_3_right))\n",
    "        input_4_right_cnn_r4_c1 = self.act_r4_c1(self.cnn_r4_c1(input_4_right))\n",
    "\n",
    "        # Downsample layer 1\n",
    "        input_1_right_cnn_r2_c1 = self.avg_r2_c1(input_1_right_cnn_r1_c1)\n",
    "        input_2_right_cnn_r3_c1 = self.avg_r3_c1(input_2_right_cnn_r2_c1)\n",
    "        input_3_right_cnn_r4_c1 = self.avg_r4_c1(input_3_right_cnn_r3_c1)\n",
    "\n",
    "        # Feature extraction for layer 2\n",
    "        input_1_right_cnn_r2_c2 = self.act_r2_c2(self.cnn_r2_c2(input_1_right_cnn_r2_c1))\n",
    "        input_2_right_cnn_r3_c2 = self.act_r3_c2(self.cnn_r3_c2(input_2_right_cnn_r3_c1))\n",
    "        input_3_right_cnn_r4_c2 = self.act_r4_c2(self.cnn_r4_c2(input_3_right_cnn_r4_c1))\n",
    "\n",
    "        # Downsample layer 2\n",
    "        input_1_right_cnn_r3_c2 = self.avg_r3_c2(input_1_right_cnn_r2_c2)\n",
    "        input_2_right_cnn_r4_c2 = self.avg_r4_c2(input_2_right_cnn_r3_c2)\n",
    "\n",
    "        # Feature extraction for layer 3\n",
    "        input_1_right_cnn_r3_c3 = self.act_r3_c3(self.cnn_r3_c3(input_1_right_cnn_r3_c2))\n",
    "        input_2_right_cnn_r4_c3 = self.act_r4_c3(self.cnn_r4_c3(input_2_right_cnn_r4_c2))\n",
    "\n",
    "        # Concatenate\n",
    "        concat_right_row_2 = torch.cat([input_2_right_cnn_r2_c1, input_1_right_cnn_r2_c2], dim=1)\n",
    "        concat_right_row_3 = torch.cat([input_3_right_cnn_r3_c1, input_2_right_cnn_r3_c2, input_1_right_cnn_r3_c3], dim=1)\n",
    "        concat_right_row_4 = torch.cat([input_4_right_cnn_r4_c1, input_3_right_cnn_r4_c2, input_2_right_cnn_r4_c3], dim=1)\n",
    "\n",
    "        # Feature extraction right side output: \n",
    "        # * input_1_right_cnn_r1_c1\n",
    "        # * concat_right_row_2\n",
    "        # * concat_right_row_3\n",
    "        # * concat_right_row_4\n",
    "        \n",
    "        # ------------- Warping features at each level\n",
    "        # Calculate the flow for each level using the input of current level and the upsampled flow from the level + 1\n",
    "        bfe_4_i1, bfe_4_i2, bfe_4_f_1_2, bfe_4_f_2_1 = self.bidirectional_warp_row_3([concat_left_row_4, concat_right_row_4, None, None])\n",
    "        bfe_3_i1, bfe_3_i2, bfe_3_f_1_2, bfe_3_f_2_1 = self.bidirectional_warp_row_3([concat_left_row_3, concat_right_row_3, bfe_4_f_1_2, bfe_4_f_2_1])\n",
    "        bfe_2_i1, bfe_2_i2, bfe_2_f_1_2, bfe_2_f_2_1 = self.bidirectional_warp_row_2([concat_left_row_2, concat_right_row_2, bfe_3_f_1_2, bfe_3_f_2_1])\n",
    "        bfe_1_i1, bfe_1_i2, _, _ = self.bidirectional_warp_row_1([input_1_left_cnn_r1_c1, input_1_right_cnn_r1_c1, bfe_2_f_1_2, bfe_2_f_2_1])\n",
    "\n",
    "        # Flow estimation output: \n",
    "        # * (bfe_1_i1, bfe_2_i1, bfe_3_i1, bfe_4_i1) \n",
    "        # * (bfe_1_i2, bfe_2_i2, bfe_3_i2, bfe_4_i2)\n",
    "        \n",
    "        # ------------- Warped features fusion   \n",
    "        # Merge row 4\n",
    "        add_row_4 = bfe_4_i1 + bfe_4_i2\n",
    "        cnn_row_4_1 = self.act_r4_1(self.cnn_r4_1(add_row_4))\n",
    "        upsample_row_4 = self.up_r4(cnn_row_4_1)\n",
    "\n",
    "        # Merge row 3\n",
    "        add_row_3 = bfe_3_i1 + bfe_3_i2 + upsample_row_4\n",
    "        cnn_row_3_1 = self.act_r3_1(self.cnn_r3_1(add_row_3))\n",
    "        upsample_row_3 = self.up_r3(cnn_row_3_1)\n",
    "\n",
    "        # Merge row 2\n",
    "        add_row_2 = bfe_2_i1 + bfe_2_i2 + upsample_row_3\n",
    "        cnn_row_2_1 = self.act_r2_1(self.cnn_r2_1(add_row_2))\n",
    "        cnn_row_2_2 = self.act_r2_2(self.cnn_r2_2(cnn_row_2_1))\n",
    "        upsample_row_2 = self.up_r2(cnn_row_2_2)\n",
    "\n",
    "        # Merge row 1\n",
    "        add_row_1 = bfe_1_i1 + bfe_1_i2 + upsample_row_2\n",
    "        cnn_row_1_1 = self.act_r1_1(self.cnn_r1_1(add_row_1))\n",
    "        cnn_row_1_2 = self.act_r1_2(self.cnn_r1_2(cnn_row_1_1))\n",
    "\n",
    "        # Create the output layer\n",
    "        fus_conv2d_outputs = self.act_out(self.cnn_out(cnn_row_1_2))\n",
    "         \n",
    "        # Feature fusion output: \n",
    "        # * fus_conv2d_outputs\n",
    "        \n",
    "        return fus_conv2d_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_1_left, input_1_right], outputs=fus_conv2d_outputs)\n",
    "model.compile(\n",
    "    loss = loss,\n",
    "    metrics = [l1, l2, psnr, ssim]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81788ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c2e3e98",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_generator, train_size, valid_generator, valid_size, optimizer, loss, metrics, epochs, batch_size, save_freq=50, log_freq=10, bad_input_limit=5, mode=\"all\"):\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss_value = loss(y, y_pred)\n",
    "            metrics_values = [metric(y, y_pred) for metric in metrics]\n",
    "\n",
    "        if tf.math.is_finite(loss_value):\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "        return loss_value, metrics_values\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def valid_step(x, y):\n",
    "        y_pred = model(x, training=False)\n",
    "        return loss(y, y_pred), [metric(y, y_pred) for metric in metrics]\n",
    "    \n",
    "    \n",
    "    def get_loss_metrics_str(loss_value, metrics_values, sep=' '):\n",
    "        result = 'loss=' + '{:.5f}'.format(loss_value)\n",
    "        for metric_value, metric in zip(metrics_values, metrics):\n",
    "            result += f'{sep}{metric.__name__}='+'{:.5f}'.format(metric_value)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    # create dict for a history and a list for bad input\n",
    "    history = {metric.__name__: [] for metric in metrics}\n",
    "    history = history | {\"val_\" + metric.__name__: [] for metric in metrics}\n",
    "    history[loss.__name__] = []\n",
    "    history[\"val_\" + loss.__name__] = []\n",
    "    bad_input = []\n",
    "    best_loss = None\n",
    "    \n",
    "    try:\n",
    "        # loop over epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            print(f\"Epoch: {epoch}/{epochs}\")\n",
    "\n",
    "            # process the full training dataset\n",
    "            total_metrics = tf.zeros(len(metrics))\n",
    "            total_loss = 0\n",
    "            batch_index = 1.0\n",
    "            for step, record in enumerate(train_generator):\n",
    "                # extract the data\n",
    "                x = record[0]\n",
    "                y = record[1]\n",
    "\n",
    "                # calculate metrics values, the loss and then apply the gradient change if loss is not NaN\n",
    "                start = time.time()\n",
    "                loss_value, metrics_values = train_step(x, y)\n",
    "                end = time.time()\n",
    "                \n",
    "                # is loss was NaN save the bad input and get to next iteration\n",
    "                if not tf.math.is_finite(loss_value):\n",
    "                    print(f\"Loss NaN detected at epoch {epoch} in step {(step+1)}. Wrong data saved to bad_input list\")\n",
    "                    bad_input.append((x, y))\n",
    "                    if len(bad_input) >= bad_input_limit:\n",
    "                        raise OverflowError(f\"The bad_input limit of {bad_input_limit} was reached\")\n",
    "                    continue\n",
    "\n",
    "                # save the loss & metrics values\n",
    "                total_loss += loss_value\n",
    "                total_metrics += metrics_values\n",
    "\n",
    "                # save the model\n",
    "                if step % save_freq == 0 and step > 0:\n",
    "                    loss_avg = total_loss / batch_index\n",
    "                    if mode == \"all\" or (mode == \"best\" and (best_loss is None or best_loss > loss_avg)):\n",
    "                        print(\"Saving model with loss \" + '{:.5f}'.format(loss_avg))\n",
    "                        model.save(os.path.join(model_base_path, f'{model_name}_{get_loss_metrics_str(loss_avg, total_metrics/batch_index, sep=\"_\")}_e={(epoch)}_s={(step+1)}_t={int(time.time())}.h5'))\n",
    "                        best_loss = loss_avg\n",
    "\n",
    "                # log the loss\n",
    "                if step % log_freq == 0:\n",
    "                    time_left = (end - start) * ((train_size // batch_size) - (step+1))\n",
    "                    time_formatted = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time_left))\n",
    "                    ljust_length = len(str(train_size//batch_size)) * 2 + 1\n",
    "                    prefix = 'Step ' + f'{(step+1)}/{(train_size//batch_size)}'.rjust(ljust_length) + f\" (eta: {time_formatted}): \"\n",
    "                    print(f'{prefix}{get_loss_metrics_str(total_loss/batch_index, total_metrics/batch_index)}')\n",
    "\n",
    "                # break the learning if the generator is over\n",
    "                if step >= ((train_size // batch_size) - 1):\n",
    "                    break\n",
    "                \n",
    "                batch_index += 1.0\n",
    "\n",
    "            # save the loss value\n",
    "            history[loss.__name__].append(total_loss / batch_index)\n",
    "            for index, metric in enumerate(metrics):\n",
    "                history[metric.__name__].append(total_metrics[index] / batch_index)\n",
    "\n",
    "            # process the full validating dataset\n",
    "            total_loss = 0\n",
    "            total_metrics = tf.zeros(len(metrics))\n",
    "            batch_index = 1.0\n",
    "            for step, record in enumerate(valid_generator):\n",
    "                x = record[0]\n",
    "                y = record[1]\n",
    "\n",
    "                loss_value, metrics_values = valid_step(x, y)\n",
    "                total_loss += loss_value\n",
    "                total_metrics += metrics_values\n",
    "\n",
    "                if step >= ((valid_size // batch_size) - 1):\n",
    "                    break\n",
    "\n",
    "                batch_index += 1.0\n",
    "\n",
    "            # log the validation score\n",
    "            print(f'Validation for epoch {epoch}: {get_loss_metrics_str(total_loss/batch_index, total_metrics/batch_index)}')\n",
    "\n",
    "            # save the validation score\n",
    "            history[\"val_\" + loss.__name__].append(total_loss/batch_index)\n",
    "            for index, metric in enumerate(metrics):\n",
    "                history[\"val_\" + metric.__name__].append(total_metrics[index]/batch_index)\n",
    "    except (OverflowError, KeyboardInterrupt) as e:\n",
    "        print(f\"Learning interrupted. Details: '{e}'\")\n",
    "    \n",
    "    return history, bad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9df836",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(\n",
    "    model=model, \n",
    "    train_generator=train_generator,\n",
    "    train_size=data_train_size, \n",
    "    valid_generator=valid_generator,\n",
    "    valid_size=data_valid_size,\n",
    "    optimizer=optimizers.Nadam(0.0001), \n",
    "    loss=loss, \n",
    "    metrics=[l1, l2, psnr, ssim],\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    save_freq=100,\n",
    "    log_freq=100,\n",
    "    bad_input_limit=50,\n",
    "    mode=\"best\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
