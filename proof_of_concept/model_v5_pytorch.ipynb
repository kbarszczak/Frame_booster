{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad822e88",
   "metadata": {},
   "source": [
    "# Proof of concept notebook for the Frame Booster project\n",
    "- Author: Kamil Barszczak\n",
    "- Contact: kamilbarszczak62@gmail.com\n",
    "- Project: https://github.com/kbarszczak/Frame_booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b91489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import torchvision\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9edbc19",
   "metadata": {},
   "source": [
    "#### Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b419997",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'E:/Data/Video_Frame_Interpolation/processed/vimeo90k_pytorch'\n",
    "data_subdir = 'data'\n",
    "train_ids = 'train.txt'\n",
    "test_ids = 'test.txt'\n",
    "valid_ids = 'valid.txt'\n",
    "\n",
    "width, height = 256, 144\n",
    "epochs = 4\n",
    "batch = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10e7d7",
   "metadata": {},
   "source": [
    "#### Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfb90a",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteImageDataset(data.Dataset):\n",
    "    def __init__(self, path, subdir, split_filename, shape):\n",
    "        self.path = path\n",
    "        self.subdir = subdir\n",
    "        self.shape = shape\n",
    "        self.ids = pd.read_csv(os.path.join(path, split_filename), names=[\"ids\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.path, self.subdir, str(self.ids.iloc[idx, 0]))\n",
    "        \n",
    "        imgs = [\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im1')),\n",
    "            self._read_bytes_to_tensor(os.path.join(img_path, 'im3'))\n",
    "        ]\n",
    "        true = self._read_bytes_to_tensor(os.path.join(img_path, 'im2'))\n",
    "        \n",
    "        return imgs, true\n",
    "    \n",
    "    def _read_bytes_to_tensor(self, path):\n",
    "        with open(path, 'rb') as bf:\n",
    "            buf = bf.read()\n",
    "            if len(buf) <= 0:\n",
    "                print(F\"Wrong buffer for {path}\")\n",
    "            return torch.permute(torch.reshape(torch.frombuffer(buf, dtype=torch.float), self.shape), (2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = train_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    shuffle = True,\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = test_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "valid_dataloader = data.DataLoader(\n",
    "    dataset = ByteImageDataset(\n",
    "        path = base_path,\n",
    "        subdir = data_subdir,\n",
    "        split_filename = valid_ids,\n",
    "        shape = (height, width, 3)\n",
    "    ),\n",
    "    batch_size = batch,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f'Training batches: {len(train_dataloader)}')\n",
    "    print(f'Testing batches: {len(test_dataloader)}')\n",
    "    print(f'Validating batches: {len(valid_dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074862e",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return 1 - psnr / 40.0\n",
    "\n",
    "    \n",
    "def loss(y_true, y_pred):\n",
    "    psnr_ = psnr(y_true, y_pred)\n",
    "    mse_ = mse(y_true, y_pred)\n",
    "    mae_ = mae(y_true, y_pred)\n",
    "    \n",
    "    return psnr_ + 5.0*mae_ + 10.0*mse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowEstimation(nn.Module):\n",
    "    def __init__(self, flow_input_chanels,\n",
    "                flow_info = {\n",
    "                    \"filter_counts\": [32, 64, 64, 16, 12, 12],\n",
    "                    \"filter_sizes\": [(3, 3), (3, 3), (3, 3), (3, 3), (1, 1), (1, 1)],\n",
    "                    \"filter_strides\": [1, 1, 1, 1, 1, 1],\n",
    "                    \"filter_padding\": [1, 1, 1, 1, 0, 0],\n",
    "                    \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()],\n",
    "                }, **kwargs):\n",
    "        super(FlowEstimation, self).__init__(**kwargs)\n",
    "\n",
    "        modules = []\n",
    "        last_output_size = flow_input_chanels\n",
    "        for fcount, fsize, fstride, fpad, fact in zip(flow_info['filter_counts'], flow_info['filter_sizes'], flow_info['filter_strides'], flow_info['filter_padding'], flow_info['activations']):\n",
    "            modules.append(nn.Conv2d(last_output_size, fcount, fsize, fstride, fpad))\n",
    "            modules.append(fact)\n",
    "            last_output_size = fcount\n",
    "\n",
    "        modules.append(nn.Conv2d(last_output_size, 2, 1))\n",
    "        self.flow = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalFeatureWarp(nn.Module):\n",
    "    def __init__(self, flow_prediction, interpolation='bilinear', **kwargs):\n",
    "        super(BidirectionalFeatureWarp, self).__init__(**kwargs)\n",
    "        \n",
    "        self.flow_prediction = flow_prediction\n",
    "        self.flow_upsample_1_2 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        self.flow_upsample_2_1 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "\n",
    "    def forward(self, input_1, input_2, flow_1_2, flow_2_1):\n",
    "        if torch.is_tensor(flow_1_2) and torch.is_tensor(flow_2_1):\n",
    "            input_1_warped_1 = BidirectionalFeatureWarp.warp(input_1, flow_1_2)\n",
    "            input_2_warped_1 = BidirectionalFeatureWarp.warp(input_2, flow_2_1)\n",
    "        else:\n",
    "            input_1_warped_1 = input_1\n",
    "            input_2_warped_1 = input_2\n",
    "            \n",
    "        flow_change_1_2_concat = torch.cat([input_2, input_1_warped_1], dim=1)\n",
    "        flow_change_1_2 = self.flow_prediction(flow_change_1_2_concat)\n",
    "        \n",
    "        flow_change_2_1_concat = torch.cat([input_1, input_2_warped_1], dim=1)\n",
    "        flow_change_2_1 = self.flow_prediction(flow_change_2_1_concat)\n",
    "        \n",
    "        if torch.is_tensor(flow_1_2) and torch.is_tensor(flow_2_1):\n",
    "            flow_1_2_changed = flow_1_2 + flow_change_1_2\n",
    "            flow_2_1_changed = flow_2_1 + flow_change_2_1\n",
    "        else:\n",
    "            flow_1_2_changed = flow_change_1_2\n",
    "            flow_2_1_changed = flow_change_2_1\n",
    "            \n",
    "        input_1_warped_2 = BidirectionalFeatureWarp.warp(input_1, flow_1_2_changed)\n",
    "        input_2_warped_2 = BidirectionalFeatureWarp.warp(input_2, flow_2_1_changed)\n",
    "        flow_1_2_changed_upsampled = self.flow_upsample_1_2(flow_1_2_changed)\n",
    "        flow_2_1_changed_upsampled = self.flow_upsample_2_1(flow_2_1_changed)\n",
    "        \n",
    "        return input_1_warped_2, input_2_warped_2, flow_1_2_changed_upsampled, flow_2_1_changed_upsampled\n",
    "    \n",
    "    @staticmethod\n",
    "    def warp(image: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = image.size()\n",
    "\n",
    "        xx = torch.arange(0, W).view(1 ,-1).repeat(H, 1)\n",
    "        yy = torch.arange(0, H).view(-1 ,1).repeat(1, W)\n",
    "        xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "        yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)\n",
    "\n",
    "        grid = torch.cat((xx, yy), 1).float()\n",
    "        if image.is_cuda:\n",
    "            grid = grid.cuda()\n",
    "\n",
    "        vgrid = grid + flow\n",
    "        vgrid[:, 0, :, :] = 2.0 * vgrid[: ,0 ,: ,:].clone() / max(W - 1, 1) - 1.0\n",
    "        vgrid[:, 1, :, :] = 2.0 * vgrid[: ,1 ,: ,:].clone() / max(H - 1, 1) - 1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0, 2, 3, 1)\n",
    "        flow = flow.permute(0, 2, 3, 1)\n",
    "        output = F.grid_sample(image, vgrid, align_corners=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape,\n",
    "                 encoder_filters = [\n",
    "                     [96, 80, 64, 64],  # encoder_filters_col_1\n",
    "                     [80, 64, 64],  # encoder_filters_col_2\n",
    "                     [64, 64]  # encoder_filters_col_3\n",
    "                 ], \n",
    "                 decoder_filters = [40, 40],  # decoder_filters\n",
    "                 flow_info = [\n",
    "                     {  # flow_1\n",
    "                        \"filter_counts\": [32, 48, 64, 80, 80, 48],\n",
    "                        \"filter_sizes\": [7, 5, 5, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1, 1, 1],\n",
    "                        \"filter_padding\": [3, 2, 2, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     }, \n",
    "                     {  # flow_2\n",
    "                        \"filter_counts\": [24, 32, 64, 64, 32],\n",
    "                        \"filter_sizes\": [5, 3, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1, 1],\n",
    "                        \"filter_padding\": [2, 1, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     }, \n",
    "                     {  # flow_3\n",
    "                        \"filter_counts\": [24, 48, 48, 16],\n",
    "                        \"filter_sizes\": [3, 3, 1, 1],\n",
    "                        \"filter_strides\": [1, 1, 1, 1],\n",
    "                        \"filter_padding\": [1, 1, 0, 0],\n",
    "                        \"activations\": [nn.PReLU(), nn.PReLU(), nn.PReLU(), nn.PReLU()]\n",
    "                     },\n",
    "                 ], interpolation=\"bilinear\", **kwargs):\n",
    "        super(FBNet, self).__init__(**kwargs)\n",
    "        \n",
    "        self.c = input_shape[0]\n",
    "        self.h = input_shape[1]\n",
    "        self.w = input_shape[2]\n",
    "        \n",
    "        # ------------- Shared Conv2d, AvgPool2d & Resize encoding layers\n",
    "        self.resize_1_2 = torchvision.transforms.Resize(size=(height//2, width//2), antialias=True)\n",
    "        self.resize_1_4 = torchvision.transforms.Resize(size=(height//4, width//4), antialias=True)\n",
    "        self.resize_1_8 = torchvision.transforms.Resize(size=(height//8, width//8), antialias=True)\n",
    "        \n",
    "        self.cnn_r1_c1 = nn.Conv2d(self.c, encoder_filters[0][0], 3, 1, 1)\n",
    "        self.cnn_r2_c1 = nn.Conv2d(self.c, encoder_filters[0][1], 3, 1, 1)\n",
    "        self.cnn_r3_c1 = nn.Conv2d(self.c, encoder_filters[0][2], 3, 1, 1)\n",
    "        self.cnn_r4_c1 = nn.Conv2d(self.c, encoder_filters[0][3], 3, 1, 1)\n",
    "        self.act_r1_c1 = nn.PReLU()\n",
    "        self.act_r2_c1 = nn.PReLU()\n",
    "        self.act_r3_c1 = nn.PReLU()\n",
    "        self.act_r4_c1 = nn.PReLU()\n",
    "\n",
    "        self.cnn_r2_c2 = nn.Conv2d(encoder_filters[0][0], encoder_filters[1][0], 3, 1, 1)\n",
    "        self.cnn_r3_c2 = nn.Conv2d(encoder_filters[0][1], encoder_filters[1][1], 3, 1, 1)\n",
    "        self.cnn_r4_c2 = nn.Conv2d(encoder_filters[0][2], encoder_filters[1][2], 3, 1, 1)\n",
    "        self.act_r2_c2 = nn.PReLU()\n",
    "        self.act_r3_c2 = nn.PReLU()\n",
    "        self.act_r4_c2 = nn.PReLU()\n",
    "\n",
    "        self.cnn_r3_c3 = nn.Conv2d(encoder_filters[1][0], encoder_filters[2][0], 3, 1, 1)\n",
    "        self.cnn_r4_c3 = nn.Conv2d(encoder_filters[1][1], encoder_filters[2][1], 3, 1, 1)\n",
    "        self.act_r3_c3 = nn.PReLU()\n",
    "        self.act_r4_c3 = nn.PReLU()\n",
    "        \n",
    "        self.avg_r2_c1 = nn.AvgPool2d(2)\n",
    "        self.avg_r3_c1 = nn.AvgPool2d(2)\n",
    "        self.avg_r4_c1 = nn.AvgPool2d(2)\n",
    "        \n",
    "        self.avg_r3_c2 = nn.AvgPool2d(2)\n",
    "        self.avg_r4_c2 = nn.AvgPool2d(2)\n",
    "        \n",
    "        # ------------- Feature warping layers \n",
    "        self.bidirectional_warp_row_1 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = 2*encoder_filters[0][0],\n",
    "                flow_info = flow_info[0]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        self.bidirectional_warp_row_2 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = 2*(encoder_filters[0][1] + encoder_filters[1][0]),\n",
    "                flow_info = flow_info[1]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        self.bidirectional_warp_row_3 = BidirectionalFeatureWarp(\n",
    "            flow_prediction = FlowEstimation(\n",
    "                flow_input_chanels = 2*(encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0]),\n",
    "                flow_info = flow_info[2]\n",
    "            ),\n",
    "            interpolation = interpolation\n",
    "        )\n",
    "        \n",
    "        # ------------- Decoding Conv2d layers\n",
    "        self.cnn_r4_1 = nn.Conv2d(encoder_filters[0][3] + encoder_filters[1][2] + encoder_filters[2][1], encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0], 3, 1, 1)\n",
    "        self.act_r4_1 = nn.PReLU()\n",
    "        self.up_r4 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r3_1 = nn.Conv2d(encoder_filters[0][2] + encoder_filters[1][1] + encoder_filters[2][0], encoder_filters[0][1] + encoder_filters[1][0], 3, 1, 1)\n",
    "        self.act_r3_1 = nn.PReLU()\n",
    "        self.up_r3 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r2_1 = nn.Conv2d(encoder_filters[0][1] + encoder_filters[1][0], encoder_filters[0][0], 3, 1, 1)\n",
    "        self.cnn_r2_2 = nn.Conv2d(encoder_filters[0][0], encoder_filters[0][0], 3, 1, 1)\n",
    "        self.act_r2_1 = nn.PReLU()\n",
    "        self.act_r2_2 = nn.PReLU()\n",
    "        self.up_r2 = nn.Upsample(scale_factor=(2, 2), mode=interpolation)\n",
    "        \n",
    "        self.cnn_r1_1 = nn.Conv2d(encoder_filters[0][0], decoder_filters[0], 3, 1, 1)\n",
    "        self.cnn_r1_2 = nn.Conv2d(decoder_filters[0], decoder_filters[1], 3, 1, 1)\n",
    "        self.act_r1_1 = nn.PReLU()\n",
    "        self.act_r1_2 = nn.PReLU()\n",
    "        \n",
    "        self.cnn_out = nn.Conv2d(decoder_filters[1], 3, 1, 1, 0)\n",
    "        self.act_out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_1_left, input_1_right):\n",
    "        # ------------- Process left input\n",
    "        input_2_left = self.resize_1_2(input_1_left)\n",
    "        input_3_left = self.resize_1_4(input_2_left)\n",
    "        input_4_left = self.resize_1_8(input_3_left)\n",
    "        \n",
    "        # Feature extraction for layer 1\n",
    "        input_1_left_cnn_r1_c1 = self.act_r1_c1(self.cnn_r1_c1(input_1_left))\n",
    "        input_2_left_cnn_r2_c1 = self.act_r2_c1(self.cnn_r2_c1(input_2_left))\n",
    "        input_3_left_cnn_r3_c1 = self.act_r3_c1(self.cnn_r3_c1(input_3_left))\n",
    "        input_4_left_cnn_r4_c1 = self.act_r4_c1(self.cnn_r4_c1(input_4_left))\n",
    "\n",
    "        # Downsample layer 1\n",
    "        input_1_left_cnn_r2_c1 = self.avg_r2_c1(input_1_left_cnn_r1_c1)\n",
    "        input_2_left_cnn_r3_c1 = self.avg_r3_c1(input_2_left_cnn_r2_c1)\n",
    "        input_3_left_cnn_r4_c1 = self.avg_r4_c1(input_3_left_cnn_r3_c1)\n",
    "\n",
    "        # Feature extraction for layer 2\n",
    "        input_1_left_cnn_r2_c2 = self.act_r2_c2(self.cnn_r2_c2(input_1_left_cnn_r2_c1))\n",
    "        input_2_left_cnn_r3_c2 = self.act_r3_c2(self.cnn_r3_c2(input_2_left_cnn_r3_c1))\n",
    "        input_3_left_cnn_r4_c2 = self.act_r4_c2(self.cnn_r4_c2(input_3_left_cnn_r4_c1))\n",
    "\n",
    "        # Downsample layer 2\n",
    "        input_1_left_cnn_r3_c2 = self.avg_r3_c2(input_1_left_cnn_r2_c2)\n",
    "        input_2_left_cnn_r4_c2 = self.avg_r4_c2(input_2_left_cnn_r3_c2)\n",
    "\n",
    "        # Feature extraction for layer 3\n",
    "        input_1_left_cnn_r3_c3 = self.act_r3_c3(self.cnn_r3_c3(input_1_left_cnn_r3_c2))\n",
    "        input_2_left_cnn_r4_c3 = self.act_r4_c3(self.cnn_r4_c3(input_2_left_cnn_r4_c2))\n",
    "\n",
    "        # Concatenate\n",
    "        concat_left_row_2 = torch.cat([input_2_left_cnn_r2_c1, input_1_left_cnn_r2_c2], dim=1)\n",
    "        concat_left_row_3 = torch.cat([input_3_left_cnn_r3_c1, input_2_left_cnn_r3_c2, input_1_left_cnn_r3_c3], dim=1)\n",
    "        concat_left_row_4 = torch.cat([input_4_left_cnn_r4_c1, input_3_left_cnn_r4_c2, input_2_left_cnn_r4_c3], dim=1)\n",
    "        \n",
    "        # Feature extraction left side output: \n",
    "        # * input_1_left_cnn_r1_c1\n",
    "        # * concat_left_row_2\n",
    "        # * concat_left_row_3\n",
    "        # * concat_left_row_4\n",
    "        \n",
    "        # ------------- Process right input\n",
    "        input_2_right = self.resize_1_2(input_1_right)\n",
    "        input_3_right = self.resize_1_4(input_2_right)\n",
    "        input_4_right = self.resize_1_8(input_3_right)\n",
    "\n",
    "        # Feature extraction for layer 1\n",
    "        input_1_right_cnn_r1_c1 = self.act_r1_c1(self.cnn_r1_c1(input_1_right))\n",
    "        input_2_right_cnn_r2_c1 = self.act_r2_c1(self.cnn_r2_c1(input_2_right))\n",
    "        input_3_right_cnn_r3_c1 = self.act_r3_c1(self.cnn_r3_c1(input_3_right))\n",
    "        input_4_right_cnn_r4_c1 = self.act_r4_c1(self.cnn_r4_c1(input_4_right))\n",
    "\n",
    "        # Downsample layer 1\n",
    "        input_1_right_cnn_r2_c1 = self.avg_r2_c1(input_1_right_cnn_r1_c1)\n",
    "        input_2_right_cnn_r3_c1 = self.avg_r3_c1(input_2_right_cnn_r2_c1)\n",
    "        input_3_right_cnn_r4_c1 = self.avg_r4_c1(input_3_right_cnn_r3_c1)\n",
    "\n",
    "        # Feature extraction for layer 2\n",
    "        input_1_right_cnn_r2_c2 = self.act_r2_c2(self.cnn_r2_c2(input_1_right_cnn_r2_c1))\n",
    "        input_2_right_cnn_r3_c2 = self.act_r3_c2(self.cnn_r3_c2(input_2_right_cnn_r3_c1))\n",
    "        input_3_right_cnn_r4_c2 = self.act_r4_c2(self.cnn_r4_c2(input_3_right_cnn_r4_c1))\n",
    "\n",
    "        # Downsample layer 2\n",
    "        input_1_right_cnn_r3_c2 = self.avg_r3_c2(input_1_right_cnn_r2_c2)\n",
    "        input_2_right_cnn_r4_c2 = self.avg_r4_c2(input_2_right_cnn_r3_c2)\n",
    "\n",
    "        # Feature extraction for layer 3\n",
    "        input_1_right_cnn_r3_c3 = self.act_r3_c3(self.cnn_r3_c3(input_1_right_cnn_r3_c2))\n",
    "        input_2_right_cnn_r4_c3 = self.act_r4_c3(self.cnn_r4_c3(input_2_right_cnn_r4_c2))\n",
    "\n",
    "        # Concatenate\n",
    "        concat_right_row_2 = torch.cat([input_2_right_cnn_r2_c1, input_1_right_cnn_r2_c2], dim=1)\n",
    "        concat_right_row_3 = torch.cat([input_3_right_cnn_r3_c1, input_2_right_cnn_r3_c2, input_1_right_cnn_r3_c3], dim=1)\n",
    "        concat_right_row_4 = torch.cat([input_4_right_cnn_r4_c1, input_3_right_cnn_r4_c2, input_2_right_cnn_r4_c3], dim=1)\n",
    "\n",
    "        # Feature extraction right side output: \n",
    "        # * input_1_right_cnn_r1_c1\n",
    "        # * concat_right_row_2\n",
    "        # * concat_right_row_3\n",
    "        # * concat_right_row_4\n",
    "        \n",
    "        # ------------- Warping features at each level\n",
    "        # Calculate the flow for each level using the input of current level and the upsampled flow from the level + 1\n",
    "        bfe_4_i1, bfe_4_i2, bfe_4_f_1_2, bfe_4_f_2_1 = self.bidirectional_warp_row_3(concat_left_row_4, concat_right_row_4, None, None)\n",
    "        bfe_3_i1, bfe_3_i2, bfe_3_f_1_2, bfe_3_f_2_1 = self.bidirectional_warp_row_3(concat_left_row_3, concat_right_row_3, bfe_4_f_1_2, bfe_4_f_2_1)\n",
    "        bfe_2_i1, bfe_2_i2, bfe_2_f_1_2, bfe_2_f_2_1 = self.bidirectional_warp_row_2(concat_left_row_2, concat_right_row_2, bfe_3_f_1_2, bfe_3_f_2_1)\n",
    "        bfe_1_i1, bfe_1_i2, _, _ = self.bidirectional_warp_row_1(input_1_left_cnn_r1_c1, input_1_right_cnn_r1_c1, bfe_2_f_1_2, bfe_2_f_2_1)\n",
    "\n",
    "        # Flow estimation output: \n",
    "        # * (bfe_1_i1, bfe_2_i1, bfe_3_i1, bfe_4_i1) \n",
    "        # * (bfe_1_i2, bfe_2_i2, bfe_3_i2, bfe_4_i2)\n",
    "        \n",
    "        # ------------- Warped features fusion   \n",
    "        # Merge row 4\n",
    "        add_row_4 = bfe_4_i1 + bfe_4_i2\n",
    "        cnn_row_4_1 = self.act_r4_1(self.cnn_r4_1(add_row_4))\n",
    "        upsample_row_4 = self.up_r4(cnn_row_4_1)\n",
    "\n",
    "        # Merge row 3\n",
    "        add_row_3 = bfe_3_i1 + bfe_3_i2 + upsample_row_4\n",
    "        cnn_row_3_1 = self.act_r3_1(self.cnn_r3_1(add_row_3))\n",
    "        upsample_row_3 = self.up_r3(cnn_row_3_1)\n",
    "\n",
    "        # Merge row 2\n",
    "        add_row_2 = bfe_2_i1 + bfe_2_i2 + upsample_row_3\n",
    "        cnn_row_2_1 = self.act_r2_1(self.cnn_r2_1(add_row_2))\n",
    "        cnn_row_2_2 = self.act_r2_2(self.cnn_r2_2(cnn_row_2_1))\n",
    "        upsample_row_2 = self.up_r2(cnn_row_2_2)\n",
    "\n",
    "        # Merge row 1\n",
    "        add_row_1 = bfe_1_i1 + bfe_1_i2 + upsample_row_2\n",
    "        cnn_row_1_1 = self.act_r1_1(self.cnn_r1_1(add_row_1))\n",
    "        cnn_row_1_2 = self.act_r1_2(self.cnn_r1_2(cnn_row_1_1))\n",
    "\n",
    "        # Create the output layer\n",
    "        fus_conv2d_outputs = self.act_out(self.cnn_out(cnn_row_1_2))\n",
    "         \n",
    "        # Feature fusion output: \n",
    "        # * fus_conv2d_outputs\n",
    "        \n",
    "        return fus_conv2d_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8749f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fbnet = FBNet(input_shape = (3, width, height)).to(device)\n",
    "torchsummary.summary(fbnet, [(3, 144, 256), (3, 144, 256)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab67d7c",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet(left, right, y, y_pred, figsize=(20, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    data = torch.cat([\n",
    "        torchvision.transforms.functional.rotate(right, 90, expand=True),\n",
    "        torchvision.transforms.functional.rotate(y_pred, 90, expand=True), \n",
    "        torchvision.transforms.functional.rotate(y, 90, expand=True), \n",
    "        torchvision.transforms.functional.rotate(left, 90, expand=True)\n",
    "    ], dim=0)\n",
    "    grid = torchvision.utils.make_grid(data, nrow=left.shape[0])\n",
    "    grid = torchvision.transforms.functional.rotate(grid, 270, expand=True)\n",
    "    plt.imshow(torch.permute(grid, (1, 2, 0)).cpu())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def norm_0_1(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def plot_history(history, norm=norm_0_1, figsize=(10,5)):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    metrics = list(history.keys())\n",
    "    metrics = [metric for metric in metrics if \"val\" not in metric]\n",
    "    \n",
    "    data = [(index, history[metric], history['val_'+metric], metric) for index, metric in enumerate(metrics)]\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    epochs = range(1, len(data[0][1]) + 1)\n",
    "    \n",
    "    for index, value, val_value, metric in data:\n",
    "        if norm is not None:\n",
    "            buffer = norm(value + val_value)\n",
    "            value = buffer[0:len(epochs)]\n",
    "            val_value = buffer[len(epochs):]\n",
    "        \n",
    "        plt.plot(epochs, value, colors[index], label=f\"train {metric}\")\n",
    "        plt.plot(epochs, val_value, colors[index]+'--', label=f\"valid {metric}\")\n",
    "        \n",
    "    plt.xticks(epochs)    \n",
    "    plt.yticks()\n",
    "    plt.title(\"Comparision of training and validating scores\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(\"Values\" if norm is None else \"Values normalized\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train, valid, optimizer, loss, metrics, epochs, batch, save_freq=500, log_freq=1, log_perf_freq=2500, mode=\"best\"):  \n",
    "    # create dict for a history\n",
    "    history = {loss.__name__: []} | {metric.__name__: [] for metric in metrics} | {'val_' + loss.__name__: []} | {\"val_\" + metric.__name__: [] for metric in metrics}\n",
    "    best_loss = None\n",
    "    \n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "        \n",
    "        # create empty dict for loss and metrics\n",
    "        loss_metrics = {loss.__name__: []} | {metric.__name__: [] for metric in metrics}\n",
    "\n",
    "        # loop over batches\n",
    "        for step, record in enumerate(train):\n",
    "            start = time.time()\n",
    "            \n",
    "            # extract the data\n",
    "            left, right, y = record[0][0].to(device), record[0][1].to(device), record[1].to(device)\n",
    "\n",
    "            # clear gradient\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            y_pred = model(left, right) \n",
    "            \n",
    "            # calculate loss and apply the gradient\n",
    "            loss_value = loss(y, y_pred)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate metrics\n",
    "            y_pred_detached = y_pred.detach()\n",
    "            metrics_values = [metric(y, y_pred_detached) for metric in metrics]\n",
    "            \n",
    "            # save the loss and metrics\n",
    "            loss_metrics[loss.__name__].append(loss_value.item())\n",
    "            for metric, value in zip(metrics, metrics_values):\n",
    "                loss_metrics[metric.__name__].append(value.item())\n",
    "                \n",
    "            end = time.time()\n",
    "            \n",
    "            # save the model\n",
    "            if step % save_freq == 0 and step > 0:\n",
    "                loss_avg = np.mean(loss_metrics[loss.__name__])\n",
    "                if mode == \"all\" or (mode == \"best\" and (best_loss is None or best_loss > loss_avg)):\n",
    "                    filename = f'../models/model_v5/fbnet_l={loss_avg}_e={epoch+1}_t={int(time.time())}.pth'\n",
    "                    torch.save(model.state_dict(), filename)\n",
    "                    \n",
    "            # log the model performance\n",
    "            if step % log_perf_freq == 0 and step > 0:\n",
    "                plot_triplet(left, right, y, y_pred.detach())\n",
    "                \n",
    "            # log the state\n",
    "            if step % log_freq == 0:\n",
    "                time_left = (end-start) * (len(train) - (step+1))\n",
    "                print('\\r[%5d/%5d] (eta: %s)' % ((step+1), len(train), time.strftime('%H:%M:%S', time.gmtime(time_left))), end='')\n",
    "                for metric, values in loss_metrics.items():\n",
    "                    print(f' {metric}=%.4f' % (np.mean(values)), end='')\n",
    "            \n",
    "        # save the training history\n",
    "        for metric, values in loss_metrics.items():\n",
    "            history[metric].append(np.mean(values))\n",
    "\n",
    "        # setup dict for validation loss and metrics\n",
    "        loss_metrics = {loss.__name__: []} | {metric.__name__: [] for metric in metrics}\n",
    "        \n",
    "        # process the full validating dataset\n",
    "        for step, record in enumerate(valid):\n",
    "            left, right, y = record[0][0].to(device), record[0][1].to(device), record[1].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(left, right).detach()\n",
    "            \n",
    "            # save the loss and metrics\n",
    "            loss_metrics[loss.__name__].append(loss(y, y_pred).item())\n",
    "            for metric, value in zip(metrics, [metric(y, y_pred) for metric in metrics]):\n",
    "                loss_metrics[metric.__name__].append(value.item())\n",
    "            \n",
    "        # log the validation score & save the validation history\n",
    "        for metric, values in loss_metrics.items():\n",
    "            metric_values_avg = np.mean(values)\n",
    "            print(f' val_{metric}=%.4f' % (metric_values_avg), end='')\n",
    "            history[f\"val_{metric}\"].append(metric_values_avg)\n",
    "            \n",
    "        # restart state printer\n",
    "        print()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bd825",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = fit(\n",
    "    model = fbnet, \n",
    "    train = train_dataloader,\n",
    "    valid = valid_dataloader,\n",
    "    optimizer = optim.NAdam(fbnet.parameters(), lr=2e-4, betas=(0.5, 0.9)), \n",
    "    loss = loss, \n",
    "    metrics = [mse, mae, psnr],\n",
    "    epochs = epochs, \n",
    "    batch = batch, \n",
    "    save_freq = 500,\n",
    "    log_freq = 1,\n",
    "    log_perf_freq = 2500,\n",
    "    mode = \"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fbnet.state_dict(), f'../models/model_v5/fbnet_e={epochs}.pth')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022c2c2",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf098604",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbnet.load_state_dict(torch.load('../models/model_v5/fbnet.pth'))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
