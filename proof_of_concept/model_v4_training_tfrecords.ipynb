{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dd1202",
   "metadata": {},
   "source": [
    "# Proof of concept notebook for the Frame Booster project\n",
    "- Author: Kamil Barszczak\n",
    "- Contact: kamilbarszczak62@gmail.com\n",
    "- Project: https://github.com/kbarszczak/Frame_booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons.image as tfa_image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "model_base_path = 'E:/OneDrive - Akademia Górniczo-Hutnicza im. Stanisława Staszica w Krakowie/Programming/Labs/Frame_booster/models/model_v4'\n",
    "model_name = 'frame_booster'\n",
    "width, height = 256, 144\n",
    "\n",
    "# training data parameters\n",
    "data_base_path = 'E:/Data/Video_Frame_Interpolation/processed/vimeo90k'\n",
    "data_creation_time = 1684271724\n",
    "data_train_size = 49500\n",
    "data_test_size = 3783\n",
    "data_valid_size = 500\n",
    "batch_size = 1\n",
    "epochs = 5\n",
    "\n",
    "# visualization data parameters\n",
    "vis_base_path = 'E:/Data/Video_Frame_Interpolation/processed/low_motion'\n",
    "vis_creation_time = '1682179015'\n",
    "vis_prefix = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6906d",
   "metadata": {},
   "source": [
    "## Load generators for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    'image_1': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_2': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_3': tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48201b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_decode_record(record):\n",
    "    features = tf.io.parse_single_example(record, name_to_features)\n",
    "    image_1 = tf.io.decode_raw(\n",
    "        features['image_1'], out_type='float32', little_endian=True, fixed_length=None, name=None\n",
    "    )\n",
    "    image_1 = tf.reshape(image_1, (height, width, 3))\n",
    "    \n",
    "    image_2 = tf.io.decode_raw(\n",
    "        features['image_2'], out_type='float32', little_endian=True, fixed_length=None, name=None\n",
    "    )\n",
    "    image_2 = tf.reshape(image_2, (height, width, 3))\n",
    "    \n",
    "    image_3 = tf.io.decode_raw(\n",
    "        features['image_3'], out_type='float32', little_endian=True, fixed_length=None, name=None\n",
    "    )\n",
    "    image_3 = tf.reshape(image_3, (height, width, 3))\n",
    "    \n",
    "    return (image_1, image_3), image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generator(base_path, prefix, creation_time, width, height, size):\n",
    "    path = os.path.join(base_path, f'{prefix}_{size}_{height}x{width}_{creation_time}.tfrecords')\n",
    "    generator = tf.data.TFRecordDataset(path)\n",
    "\n",
    "    generator = generator.map(parse_decode_record)\n",
    "    generator = generator.repeat(epochs)\n",
    "    generator = generator.prefetch(5)\n",
    "    generator = generator.shuffle(buffer_size=5 * batch_size)\n",
    "    generator = generator.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = load_generator(\n",
    "    base_path = data_base_path, \n",
    "    prefix = 'train',\n",
    "    creation_time = str(data_creation_time), \n",
    "    width = width, \n",
    "    height = height, \n",
    "    size = str(data_train_size)\n",
    ")\n",
    "test_generator = load_generator(\n",
    "    base_path = data_base_path, \n",
    "    prefix = 'test',\n",
    "    creation_time = str(data_creation_time),\n",
    "    width = width, \n",
    "    height = height, \n",
    "    size = str(data_test_size)\n",
    ")\n",
    "valid_generator = load_generator(\n",
    "    base_path = data_base_path, \n",
    "    prefix = 'valid',\n",
    "    creation_time = str(data_creation_time),\n",
    "    width = width, \n",
    "    height = height, \n",
    "    size = str(data_valid_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432aaeb",
   "metadata": {},
   "source": [
    "## Load data for test visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_path, prefix, creation_time, width, height):\n",
    "    with open(os.path.join(base_path, f'x_{prefix}_{height}x{width}_{creation_time}.pickle'), 'rb') as file:\n",
    "        x = pickle.load(file)\n",
    "        \n",
    "    with open(os.path.join(base_path, f'y_{prefix}_{height}x{width}_{creation_time}.pickle'), 'rb') as file:\n",
    "        y = pickle.load(file)\n",
    "        \n",
    "    return (np.array(x)/255.0).astype('float32'), (np.array(y)/255.0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4700c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vis, y_vis = load_data(\n",
    "    base_path = vis_base_path,\n",
    "    prefix = vis_prefix,\n",
    "    creation_time = vis_creation_time,\n",
    "    width = width, \n",
    "    height = height\n",
    ")\n",
    "if len(x_vis) % batch_size != 0:\n",
    "    x_vis = x_vis[0:len(x_vis) - (len(x_vis)%batch_size)]\n",
    "    y_vis = y_vis[0:len(x_vis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92349fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_generator = ImageDataGenerator().flow(\n",
    "    x = [x_vis[:, 0, :, :], x_vis[:, 1, :, :]],\n",
    "    y = y_vis,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    ")\n",
    "x_vis, y_vis = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bbc7d",
   "metadata": {},
   "source": [
    "## Create activation and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The output activation returns linear values cropped to range from 0 to 1\n",
    "\"\"\"\n",
    "def output_activation(x):\n",
    "    return tf.math.minimum(tf.math.maximum(x, 0), 1)\n",
    "\n",
    "\"\"\"\n",
    "The L1 reconstruction loss function makes the final image colors \n",
    "look the same as the colors in the ground-truth image\n",
    "\"\"\"\n",
    "def l1(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The L2 reconstruction loss function is similar to L1\n",
    "\"\"\"\n",
    "def l2(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The PSNR loss function is responsible for boosting the overall quality of the image by reducing its noise \n",
    "(The higher the PSNR the better so we return 1 - PSNR because the loss function tries to minimize it)\n",
    "\"\"\"\n",
    "def psnr(y_true, y_pred):\n",
    "    psnr = tf.math.reduce_mean(tf.image.psnr(y_true, y_pred, max_val = 1.0))\n",
    "    return 1 - psnr / 40.0\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The SSIM loss function keeps the result image structure\n",
    "(The more significant the SSIM the more similar the final image is)\n",
    "\"\"\"\n",
    "def ssim(y_true, y_pred):\n",
    "    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "    return 1 - ssim\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "The final loss is linear combination of ssim, psnr, l1, l2 losses\n",
    "\"\"\"\n",
    "def loss(y_true, y_pred):\n",
    "    ssim_ = ssim(y_true, y_pred)\n",
    "    psnr_ = psnr(y_true, y_pred)\n",
    "    l1_ = l1(y_true, y_pred)\n",
    "    l2_ = l2(y_true, y_pred)\n",
    "    \n",
    "    return ssim_ + psnr_ + 5.0*l1_ + 10.0*l2_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444234c",
   "metadata": {},
   "source": [
    "## Create and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56388204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalFlowEstimation(layers.Layer):\n",
    "    def __init__(self, filter_count=[32, 64, 64, 16], filter_size=[(3, 3), (3, 3), (1, 1), (1, 1)], activation='relu', regularizer=None, interpolation='bilinear', **kwargs):\n",
    "        super(BidirectionalFlowEstimation, self).__init__(**kwargs)\n",
    "        \n",
    "        # flow 1 -> 2\n",
    "        self.flow_add_1_2 = layers.Add()\n",
    "        self.flow_upsample_1_2 = layers.UpSampling2D((2, 2), interpolation=interpolation)\n",
    "        self.flow_1_2_concat = layers.Concatenate(axis=3)\n",
    "        \n",
    "        # flow 2 -> 1\n",
    "        self.flow_add_2_1 = layers.Add()\n",
    "        self.flow_upsample_2_1 = layers.UpSampling2D((2, 2), interpolation=interpolation)\n",
    "        self.flow_2_1_concat = layers.Concatenate(axis=3)\n",
    "        \n",
    "        # flow estimation sequentails layers\n",
    "        self.flow_prediction =  keras.Sequential()\n",
    "        for fc, fs in zip(filter_count, filter_size):\n",
    "            self.flow_prediction.add(\n",
    "                layers.Conv2D(fc, fs, activation=activation, kernel_regularizer=regularizer, padding='same')\n",
    "            )\n",
    "        self.flow_prediction.add(layers.Conv2D(2, (1, 1), kernel_regularizer=regularizer, padding='same'))\n",
    "        \n",
    "        self.filter_count = filter_count\n",
    "        self.filter_size = filter_size\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.interpolation = interpolation\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"filter_count\": self.filter_count,\n",
    "            \"filter_size\": self.filter_size,\n",
    "            \"activation\": self.activation,\n",
    "            \"regularizer\": self.regularizer,\n",
    "            \"interpolation\": self.interpolation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_1 = inputs[0]\n",
    "        input_2 = inputs[1]\n",
    "        flow_1_2 = inputs[2]\n",
    "        flow_2_1 = inputs[3]\n",
    "\n",
    "        if not tf.is_tensor(flow_1_2) or not tf.is_tensor(flow_2_1):\n",
    "            flow_1_2 = tf.zeros(shape=(batch_size, input_2.shape[1], input_2.shape[2], 2))\n",
    "            flow_2_1 = tf.zeros(shape=(batch_size, input_2.shape[1], input_2.shape[2], 2))\n",
    "        \n",
    "        # input_1 to input_2 flow prediction\n",
    "        input_1_warped_1 = BidirectionalFlowEstimation.warp(input_1, flow_1_2)\n",
    "            \n",
    "        flow_change_1_2_concat = self.flow_1_2_concat([input_2, input_1_warped_1])\n",
    "        flow_change_1_2 = self.flow_prediction(flow_change_1_2_concat)\n",
    "        \n",
    "        flow_1_2_changed = self.flow_add_1_2([flow_1_2, flow_change_1_2])\n",
    "        input_1_warped_2 = BidirectionalFlowEstimation.warp(input_1, flow_1_2_changed)\n",
    "        flow_1_2_changed_upsampled = self.flow_upsample_1_2(flow_1_2_changed)\n",
    "        \n",
    "        # input_2 to input_1 flow prediction\n",
    "        input_2_warped_1 = BidirectionalFlowEstimation.warp(input_2, flow_2_1)\n",
    "        \n",
    "        flow_change_2_1_concat = self.flow_2_1_concat([input_1, input_2_warped_1])\n",
    "        flow_change_2_1 = self.flow_prediction(flow_change_2_1_concat)\n",
    "\n",
    "        flow_2_1_changed = self.flow_add_2_1([flow_2_1, flow_change_2_1])\n",
    "        input_2_warped_2 = BidirectionalFlowEstimation.warp(input_2, flow_2_1_changed)\n",
    "        flow_2_1_changed_upsampled = self.flow_upsample_2_1(flow_2_1_changed)\n",
    "        \n",
    "        return input_1_warped_2, input_2_warped_2, flow_1_2_changed_upsampled, flow_2_1_changed_upsampled\n",
    "    \n",
    "    @staticmethod\n",
    "    def warp(image: tf.Tensor, flow: tf.Tensor) -> tf.Tensor:\n",
    "        warped = tf.keras.layers.Lambda(\n",
    "            lambda x: tfa_image.dense_image_warp(*x))((image, -flow))\n",
    "        return tf.reshape(warped, shape=tf.shape(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aabcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "filter_count_col_1 = [144, 72, 48, 48]\n",
    "filter_count_col_2 = [72, 48, 48]\n",
    "filter_count_col_3 = [48, 48]\n",
    "filter_size=(3, 3)\n",
    "\n",
    "# todo:\n",
    "# - reform filters in decoding\n",
    "# - apply recursion/attension in fusion\n",
    "\n",
    "flow_1_filter_count=[32, 48, 64, 80, 80, 48] \n",
    "flow_1_filter_size=[(7, 7), (5, 5), (5, 5), (3, 3), (1, 1), (1, 1)]\n",
    "\n",
    "# flow_2_filter_count=[24, 48, 64, 64, 32] \n",
    "# flow_2_filter_size=[(5, 5), (5, 5), (3, 3), (1, 1), (1, 1)]\n",
    "\n",
    "# flow_3_filter_count=[24, 48, 48, 16] \n",
    "# flow_3_filter_size=[(3, 3), (3, 3), (1, 1), (1, 1)]\n",
    "\n",
    "# flow_4_filter_count=[16, 32, 32, 8] \n",
    "# flow_4_filter_size=[(3, 3), (3, 3), (1, 1), (1, 1)]\n",
    "\n",
    "interpolation='bilinear'\n",
    "activation=layers.LeakyReLU(0.2)\n",
    "regularizer=None\n",
    "\n",
    "# ------------- Shared conv2d layers\n",
    "cnn_r1_c1 = layers.Conv2D(filter_count_col_1[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r1_c1\")\n",
    "cnn_r2_c1 = layers.Conv2D(filter_count_col_1[1], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r2_c1\")\n",
    "cnn_r3_c1 = layers.Conv2D(filter_count_col_1[2], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r3_c1\")\n",
    "cnn_r4_c1 = layers.Conv2D(filter_count_col_1[3], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r4_c1\")\n",
    "\n",
    "cnn_r2_c2 = layers.Conv2D(filter_count_col_2[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r2_c2\")\n",
    "cnn_r3_c2 = layers.Conv2D(filter_count_col_2[1], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r3_c2\")\n",
    "cnn_r4_c2 = layers.Conv2D(filter_count_col_2[2], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r4_c2\")\n",
    "\n",
    "cnn_r3_c3 = layers.Conv2D(filter_count_col_3[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r3_c3\")\n",
    "cnn_r4_c3 = layers.Conv2D(filter_count_col_3[1], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name=\"fe_conv2d_r4_c3\")\n",
    "\n",
    "# ------------- Feature extraction left side\n",
    "input_1_left = layers.Input(shape=(height, width, 3), name=\"input_left\")\n",
    "input_2_left = tf.image.resize(input_1_left, size=(height//2, width//2), method=interpolation)\n",
    "input_3_left = tf.image.resize(input_2_left, size=(height//4, width//4), method=interpolation)\n",
    "input_4_left = tf.image.resize(input_3_left, size=(height//8, width//8), method=interpolation)\n",
    "\n",
    "# Feature extraction for layer 1\n",
    "input_1_left_cnn_r1_c1 = cnn_r1_c1(input_1_left)\n",
    "input_2_left_cnn_r2_c1 = cnn_r2_c1(input_2_left)\n",
    "input_3_left_cnn_r3_c1 = cnn_r3_c1(input_3_left)\n",
    "input_4_left_cnn_r4_c1 = cnn_r4_c1(input_4_left)\n",
    "\n",
    "# Downsample layer 1\n",
    "input_1_left_cnn_r2_c1 = layers.AveragePooling2D((2, 2), name=\"input_left_avg_r2_c1\")(input_1_left_cnn_r1_c1)\n",
    "input_2_left_cnn_r3_c1 = layers.AveragePooling2D((2, 2), name=\"input_left_avg_r3_c1\")(input_2_left_cnn_r2_c1)\n",
    "input_3_left_cnn_r4_c1 = layers.AveragePooling2D((2, 2), name=\"input_left_avg_r4_c1\")(input_3_left_cnn_r3_c1)\n",
    "\n",
    "# Feature extraction for layer 2\n",
    "input_1_left_cnn_r2_c2 = cnn_r2_c2(input_1_left_cnn_r2_c1)\n",
    "input_2_left_cnn_r3_c2 = cnn_r3_c2(input_2_left_cnn_r3_c1)\n",
    "input_3_left_cnn_r4_c2 = cnn_r4_c2(input_3_left_cnn_r4_c1)\n",
    "\n",
    "# Downsample layer 2\n",
    "input_1_left_cnn_r3_c2 = layers.AveragePooling2D((2, 2), name=\"input_left_avg_r3_c2\")(input_1_left_cnn_r2_c2)\n",
    "input_2_left_cnn_r4_c2 = layers.AveragePooling2D((2, 2), name=\"input_left_avg_r4_c2\")(input_2_left_cnn_r3_c2)\n",
    "\n",
    "# Feature extraction for layer 3\n",
    "input_1_left_cnn_r3_c3 = cnn_r3_c3(input_1_left_cnn_r3_c2)\n",
    "input_2_left_cnn_r4_c3 = cnn_r4_c3(input_2_left_cnn_r4_c2)\n",
    "\n",
    "# Concatenate\n",
    "concat_left_row_2 = layers.Concatenate(name=\"concat_left_row_2\")([input_2_left_cnn_r2_c1, input_1_left_cnn_r2_c2])\n",
    "concat_left_row_3 = layers.Concatenate(name=\"concat_left_row_3\")([input_3_left_cnn_r3_c1, input_2_left_cnn_r3_c2, input_1_left_cnn_r3_c3])\n",
    "concat_left_row_4 = layers.Concatenate(name=\"concat_left_row_4\")([input_4_left_cnn_r4_c1, input_3_left_cnn_r4_c2, input_2_left_cnn_r4_c3])\n",
    "\n",
    "# Feature extraction left side output: \n",
    "# * input_1_left_cnn_r1_c1\n",
    "# * concat_left_row_2\n",
    "# * concat_left_row_3\n",
    "# * concat_left_row_4\n",
    "\n",
    "# ------------- Feature extraction right side\n",
    "input_1_right = layers.Input(shape=(height, width, 3), name=\"input_right\")\n",
    "input_2_right = tf.image.resize(input_1_right, size=(height//2, width//2), method=interpolation)\n",
    "input_3_right = tf.image.resize(input_2_right, size=(height//4, width//4), method=interpolation)\n",
    "input_4_right = tf.image.resize(input_3_right, size=(height//8, width//8), method=interpolation)\n",
    "\n",
    "# Feature extraction for layer 1\n",
    "input_1_right_cnn_r1_c1 = cnn_r1_c1(input_1_right)\n",
    "input_2_right_cnn_r2_c1 = cnn_r2_c1(input_2_right)\n",
    "input_3_right_cnn_r3_c1 = cnn_r3_c1(input_3_right)\n",
    "input_4_right_cnn_r4_c1 = cnn_r4_c1(input_4_right)\n",
    "\n",
    "# Downsample layer 1\n",
    "input_1_right_cnn_r2_c1 = layers.AveragePooling2D((2, 2), name=\"input_right_avg_r2_c1\")(input_1_right_cnn_r1_c1)\n",
    "input_2_right_cnn_r3_c1 = layers.AveragePooling2D((2, 2), name=\"input_right_avg_r3_c1\")(input_2_right_cnn_r2_c1)\n",
    "input_3_right_cnn_r4_c1 = layers.AveragePooling2D((2, 2), name=\"input_right_avg_r4_c1\")(input_3_right_cnn_r3_c1)\n",
    "\n",
    "# Feature extraction for layer 2\n",
    "input_1_right_cnn_r2_c2 = cnn_r2_c2(input_1_right_cnn_r2_c1)\n",
    "input_2_right_cnn_r3_c2 = cnn_r3_c2(input_2_right_cnn_r3_c1)\n",
    "input_3_right_cnn_r4_c2 = cnn_r4_c2(input_3_right_cnn_r4_c1)\n",
    "\n",
    "# Downsample layer 2\n",
    "input_1_right_cnn_r3_c2 = layers.AveragePooling2D((2, 2), name=\"input_right_avg_r3_c2\")(input_1_right_cnn_r2_c2)\n",
    "input_2_right_cnn_r4_c2 = layers.AveragePooling2D((2, 2), name=\"input_right_avg_r4_c2\")(input_2_right_cnn_r3_c2)\n",
    "\n",
    "# Feature extraction for layer 3\n",
    "input_1_right_cnn_r3_c3 = cnn_r3_c3(input_1_right_cnn_r3_c2)\n",
    "input_2_right_cnn_r4_c3 = cnn_r4_c3(input_2_right_cnn_r4_c2)\n",
    "\n",
    "# Concatenate\n",
    "concat_right_row_2 = layers.Concatenate(name=\"concat_right_row_2\")([input_2_right_cnn_r2_c1, input_1_right_cnn_r2_c2])\n",
    "concat_right_row_3 = layers.Concatenate(name=\"concat_right_row_3\")([input_3_right_cnn_r3_c1, input_2_right_cnn_r3_c2, input_1_right_cnn_r3_c3])\n",
    "concat_right_row_4 = layers.Concatenate(name=\"concat_right_row_4\")([input_4_right_cnn_r4_c1, input_3_right_cnn_r4_c2, input_2_right_cnn_r4_c3])\n",
    "\n",
    "# Feature extraction right side output: \n",
    "# * input_1_right_cnn_r1_c1\n",
    "# * concat_right_row_2\n",
    "# * concat_right_row_3\n",
    "# * concat_right_row_4\n",
    "\n",
    "# Flow estimation layers     \n",
    "bidirectional_flow_estimation_row_1 = BidirectionalFlowEstimation(\n",
    "    filter_count=flow_1_filter_count, \n",
    "    filter_size=flow_1_filter_size, \n",
    "    activation=activation, \n",
    "    regularizer=regularizer, \n",
    "    interpolation=interpolation,\n",
    "    name=\"bflow_row_1\"\n",
    ")\n",
    "# bidirectional_flow_estimation_row_2 = BidirectionalFlowEstimation(\n",
    "#     filter_count=flow_2_filter_count, \n",
    "#     filter_size=flow_2_filter_size, \n",
    "#     activation=activation, \n",
    "#     regularizer=regularizer, \n",
    "#     interpolation=interpolation,\n",
    "#     name=\"bflow_row_2\"\n",
    "# )\n",
    "# bidirectional_flow_estimation_row_3 = BidirectionalFlowEstimation(\n",
    "#     filter_count=flow_3_filter_count, \n",
    "#     filter_size=flow_3_filter_size, \n",
    "#     activation=activation, \n",
    "#     regularizer=regularizer, \n",
    "#     interpolation=interpolation,\n",
    "#     name=\"bflow_row_3\"\n",
    "# )\n",
    "# bidirectional_flow_estimation_row_4 = BidirectionalFlowEstimation(\n",
    "#     filter_count=flow_4_filter_count, \n",
    "#     filter_size=flow_4_filter_size, \n",
    "#     activation=activation, \n",
    "#     regularizer=regularizer, \n",
    "#     interpolation=interpolation,\n",
    "#     name=\"bflow_row_4\"\n",
    "# )\n",
    "\n",
    "# ------------- Warping features at each level\n",
    "empty_flow_1 = tf.zeros(shape=(batch_size, height//8, width//8, 2))\n",
    "empty_flow_2 = tf.zeros(shape=(batch_size, height//8, width//8, 2))\n",
    "\n",
    "# Calculate the flow for each level using the input of current level and the upsampled flow from the level + 1\n",
    "bfe_4_i1, bfe_4_i2, bfe_4_f_1_2, bfe_4_f_2_1 = bidirectional_flow_estimation_row_1([concat_left_row_4, concat_right_row_4, empty_flow_1, empty_flow_2])\n",
    "bfe_3_i1, bfe_3_i2, bfe_3_f_1_2, bfe_3_f_2_1 = bidirectional_flow_estimation_row_1([concat_left_row_3, concat_right_row_3, bfe_4_f_1_2, bfe_4_f_2_1])\n",
    "bfe_2_i1, bfe_2_i2, bfe_2_f_1_2, bfe_2_f_2_1 = bidirectional_flow_estimation_row_1([concat_left_row_2, concat_right_row_2, bfe_3_f_1_2, bfe_3_f_2_1])\n",
    "bfe_1_i1, bfe_1_i2, _, _ = bidirectional_flow_estimation_row_1([input_1_left_cnn_r1_c1, input_1_right_cnn_r1_c1, bfe_2_f_1_2, bfe_2_f_2_1])\n",
    "\n",
    "# Flow estimation output: \n",
    "# * (bfe_1_i1, bfe_2_i1, bfe_3_i1, bfe_4_i1) \n",
    "# * (bfe_1_i2, bfe_2_i2, bfe_3_i2, bfe_4_i2)\n",
    "\n",
    "# ------------- Warped features fusion   \n",
    "        \n",
    "# merge row 4\n",
    "add_row_4 = layers.Add(name='add_row_4')([bfe_4_i1, bfe_4_i2])\n",
    "cnn_row_4_1 = layers.Conv2D(filter_count_col_1[2] + filter_count_col_2[1] + filter_count_col_3[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_4_1')(add_row_4)\n",
    "upsample_row_4 = layers.UpSampling2D((2, 2), interpolation=interpolation, name=\"upsample_row_4\")(cnn_row_4_1)\n",
    "\n",
    "# merge row 3\n",
    "add_row_3 = layers.Add(name='add_row_3')([bfe_3_i1, bfe_3_i2, upsample_row_4])\n",
    "cnn_row_3_1 = layers.Conv2D(filter_count_col_1[1] + filter_count_col_2[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_3_1')(add_row_3)\n",
    "upsample_row_3 = layers.UpSampling2D((2, 2), interpolation=interpolation, name=\"upsample_row_3\")(cnn_row_3_1)\n",
    "\n",
    "# merge row 2\n",
    "add_row_2 = layers.Add(name='add_row_2')([bfe_2_i1, bfe_2_i2, upsample_row_3])\n",
    "cnn_row_2_1 = layers.Conv2D(filter_count_col_1[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_2_1')(add_row_2)\n",
    "cnn_row_2_2 = layers.Conv2D(filter_count_col_1[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_2_2')(cnn_row_2_1)\n",
    "upsample_row_2 = layers.UpSampling2D((2, 2), interpolation=interpolation, name=\"upsample_row_2\")(cnn_row_2_2)\n",
    "\n",
    "# merge row 1\n",
    "add_row_1 = layers.Add(name='add_row_1')([bfe_1_i1, bfe_1_i2, upsample_row_2])\n",
    "cnn_row_1_1 = layers.Conv2D(filter_count_col_1[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_1_1')(add_row_1)\n",
    "cnn_row_1_2 = layers.Conv2D(filter_count_col_1[0], filter_size, activation=activation, kernel_regularizer=regularizer, padding='same', name='fus_conv2d_row_1_2')(cnn_row_1_1)\n",
    "fus_conv2d_outputs = layers.Conv2D(3, (1, 1), activation=output_activation, padding='same', name=\"fus_conv2d_outputs\")(cnn_row_1_2)\n",
    "\n",
    "model = keras.Model(inputs=[input_1_left, input_1_right], outputs=fus_conv2d_outputs)\n",
    "model.compile(\n",
    "    loss = loss,\n",
    "    metrics = [l1, l2, psnr, ssim]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70e32a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ceaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_generator, train_size, valid_generator, valid_size, optimizer, loss, metrics, epochs, batch_size, save_freq=50, log_freq=10, bad_input_limit=5, mode=\"all\"):\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss_value = loss(y, y_pred)\n",
    "\n",
    "        if tf.math.is_finite(loss_value):\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        return loss_value, y_pred\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def calc_metrics(x, y):\n",
    "        y_pred = model(x, training=False)\n",
    "        return [metric(y, y_pred) for metric in metrics]\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def valid_step(x, y):\n",
    "        y_pred = model(x, training=False)\n",
    "        return loss(y, y_pred)\n",
    "    \n",
    "    \n",
    "    def get_loss_metrics_str(loss_value, metrics_values, sep=' '):\n",
    "        result = 'loss=' + '{:.5f}'.format(loss_value)\n",
    "        for metric_value, metric in zip(metrics_values, metrics):\n",
    "            result += f'{sep}{metric.__name__}='+'{:.5f}'.format(metric_value)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    # create dict for a history and a list for bad input\n",
    "    history = {metric.__name__: [] for metric in metrics}\n",
    "    history = history | {\"val_\" + metric.__name__: [] for metric in metrics}\n",
    "    history[loss.__name__] = []\n",
    "    history[\"val_\" + loss.__name__] = []\n",
    "    bad_input = []\n",
    "    best_loss = None\n",
    "    \n",
    "    try:\n",
    "        # loop over epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            print(f\"Epoch: {epoch}/{epochs}\")\n",
    "\n",
    "            # process the full training dataset\n",
    "            total_metrics = np.zeros(len(metrics))\n",
    "            total_loss = 0\n",
    "            batch_index = 1.0\n",
    "            for step, record in enumerate(train_generator):\n",
    "                # extract the data\n",
    "                x = record[0]\n",
    "                y = record[1]\n",
    "\n",
    "                # calculate metrics values, the loss and then apply the gradient change if loss is not NaN\n",
    "                loss_value, y_pred = train_step(x, y)\n",
    "                metrics_values = np.array(calc_metrics(x, y))\n",
    "                \n",
    "                # is loss was NaN save the bad input and get to next iteration\n",
    "                if not tf.math.is_finite(loss_value):\n",
    "                    print(f\"Loss NaN detected at epoch {epoch} in step {(step+1)}. Wrong data saved to bad_input list\")\n",
    "                    bad_input.append((x, y, y_pred))\n",
    "                    if len(bad_input) >= bad_input_limit:\n",
    "                        raise OverflowError(f\"The bad_input limit of {bad_input_limit} was reached\")\n",
    "                    continue\n",
    "\n",
    "                # save the loss & metrics values\n",
    "                total_loss += loss_value\n",
    "                total_metrics += metrics_values\n",
    "\n",
    "                # save the model\n",
    "                if step % save_freq == 0 and step > 0:\n",
    "                    loss_avg = total_loss / batch_index\n",
    "                    if mode == \"all\" or (mode == \"best\" and (best_loss is None or best_loss > loss_avg)):\n",
    "                        print(\"Saving model with loss \" + '{:.5f}'.format(loss_avg))\n",
    "                        model.save(os.path.join(model_base_path, f'{model_name}_{get_loss_metrics_str(loss_avg, total_metrics/batch_index, sep=\"_\")}_e={(epoch)}_s={(step+1)}_t={int(time.time())}.h5'))\n",
    "                        best_loss = loss_avg\n",
    "\n",
    "                # log the loss\n",
    "                if step % log_freq == 0:\n",
    "                    ljust_length = len(str(train_size//batch_size)) * 2 + 3\n",
    "                    prefix = 'Step ' + f'{(step+1)}/{(train_size//batch_size)}: '.rjust(ljust_length)\n",
    "                    print(f'{prefix}{get_loss_metrics_str(total_loss/batch_index, total_metrics/batch_index)}')\n",
    "\n",
    "                # break the learning if the generator is over\n",
    "                if step >= ((train_size // batch_size) - 1):\n",
    "                    break\n",
    "                \n",
    "                batch_index += 1.0\n",
    "\n",
    "            # save the loss value\n",
    "            history[loss.__name__].append(total_loss / batch_index)\n",
    "            for index, metric in enumerate(metrics):\n",
    "                history[metric.__name__].append(total_metrics[index] / batch_index)\n",
    "\n",
    "            # process the full validating dataset\n",
    "            total_loss = 0\n",
    "            total_metrics = np.zeros(len(metrics))\n",
    "            batch_index = 1.0\n",
    "            for step, record in enumerate(valid_generator):\n",
    "                x = record[0]\n",
    "                y = record[1]\n",
    "\n",
    "                total_loss += valid_step(x, y)\n",
    "                total_metrics += np.array(calc_metrics(x, y))\n",
    "\n",
    "                if step >= ((valid_size // batch_size) - 1):\n",
    "                    break\n",
    "\n",
    "                batch_index += 1.0\n",
    "\n",
    "            # log the validation score\n",
    "            print(f'Validation for epoch {epoch}: {get_loss_metrics_str(total_loss/batch_index, total_metrics/batch_index)}')\n",
    "\n",
    "            # save the validation score\n",
    "            history[\"val_\" + loss.__name__].append(total_loss/batch_index)\n",
    "            for index, metric in enumerate(metrics):\n",
    "                history[\"val_\" + metric.__name__].append(total_metrics[index]/batch_index)\n",
    "    except (OverflowError, KeyboardInterrupt) as e:\n",
    "        print(f\"Learning interrupted. Details: '{e}'\")\n",
    "    \n",
    "    return history, bad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551b9ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history, bad_input = fit(\n",
    "    model=model, \n",
    "    train_generator=train_generator,\n",
    "    train_size=data_train_size, \n",
    "    valid_generator=valid_generator,\n",
    "    valid_size=data_valid_size,\n",
    "    optimizer=optimizers.Nadam(0.0001), \n",
    "    loss=loss, \n",
    "    metrics=[l1, l2, psnr, ssim],\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    save_freq=100,\n",
    "    log_freq=50,\n",
    "    bad_input_limit=50,\n",
    "    mode=\"best\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223c125",
   "metadata": {},
   "source": [
    "## Evaluate the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f764e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_0_1(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def norm(data):\n",
    "    return (data - np.mean(data)) / np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854788cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, normalize_method=None, metrics_restrictions=None):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(25,10))\n",
    "    \n",
    "    metrics = list(history.keys())\n",
    "    metrics = [metric for metric in metrics if \"val\" not in metric]\n",
    "    if metrics_restrictions is not None:\n",
    "        metrics = [metric for metric in metrics if metric in metrics_restrictions]\n",
    "    \n",
    "    data = [(index, history[metric], history['val_'+metric], metric) for index, metric in enumerate(metrics)]\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    epochs = range(1, len(data[0][1]) + 1)\n",
    "    \n",
    "    for index, value, val_value, metric in data:\n",
    "        if normalize_method is not None:\n",
    "            buffer = value.copy()\n",
    "            buffer.extend(val_value)\n",
    "            buffer = normalize_method(buffer)\n",
    "            value = buffer[0:len(epochs)]\n",
    "            val_value = buffer[len(epochs)::]\n",
    "        \n",
    "        plt.plot(epochs, value, colors[index], label=f\"Training {metric}\")\n",
    "        plt.plot(epochs, val_value, colors[index]+'--', label=f\"Validation {metric}\")\n",
    "        \n",
    "    plt.xticks(epochs, size=17)    \n",
    "    plt.yticks(size=17)\n",
    "    plt.title(f\"Comparison of Training and Validation metrics\", size=20)\n",
    "    plt.xlabel('Epochs', size=17)\n",
    "    plt.ylabel(\"Metric values\", size=17)\n",
    "    plt.legend(loc='upper right', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf081a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, generator, verbose=0):\n",
    "    result_dict = {}\n",
    "    result = model.evaluate(generator, verbose=verbose, steps=data_test_size//batch_size)\n",
    "    for index, metric in enumerate(model.metrics):\n",
    "        result_dict[metric.name] = result[index]\n",
    "        print(f'{metric.name.zfill(13).replace(\"0\", \" \")}: {np.round(result[index], 4)}')\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, normalize_method=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d603be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, normalize_method=norm_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fff560",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, normalize_method=None, metrics_restrictions=['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate(model, test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75696615",
   "metadata": {},
   "source": [
    "## Visualize generated frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337aecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(vis_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(generator, predictions, batch, index):\n",
    "    # verify arguments\n",
    "    batch_size = generator.batch_size\n",
    "    assert generator.shuffle == False\n",
    "    assert batch >= 0 and batch < len(generator)\n",
    "    assert index >= 0 and index < batch_size\n",
    "    \n",
    "    # get neighbours frames\n",
    "    neighbours, true = generator[batch]\n",
    "    neighbours = np.array(neighbours)\n",
    "    neighbours = neighbours[:, index, :, :, :]\n",
    "\n",
    "    # get true and predicted frames\n",
    "    true = np.array(true)[index]\n",
    "    predicted = predictions[batch_size*batch + index]\n",
    "    \n",
    "    # mark true edges on predicted frame\n",
    "    true_edges = cv2.cvtColor(true, cv2.COLOR_RGB2GRAY)\n",
    "    true_edges = cv2.GaussianBlur(true_edges, (3, 3), 1)\n",
    "    true_edges = cv2.medianBlur(true_edges, 3)\n",
    "    true_edges = cv2.Canny((true_edges*255).astype('uint8'), 50, 100)\n",
    "    predicted_marked = predicted.copy()\n",
    "    predicted_marked[true_edges != 0] = (1, 1, 1)\n",
    "\n",
    "    # plot images\n",
    "    f, ax = plt.subplots(3, 2)\n",
    "    f.set_size_inches(20, 20)\n",
    "\n",
    "    ax[0][0].set_title(\"First frame\")\n",
    "    ax[0][0].set_xticks([])\n",
    "    ax[0][0].set_yticks([])\n",
    "    ax[0][0].imshow(neighbours[0])\n",
    "    \n",
    "    ax[1][0].set_title(\"Predicted frame\")\n",
    "    ax[1][0].set_xticks([])\n",
    "    ax[1][0].set_yticks([])\n",
    "    ax[1][0].imshow(predicted)\n",
    "    \n",
    "    ax[2][0].set_title(\"Second frame\")\n",
    "    ax[2][0].set_xticks([])\n",
    "    ax[2][0].set_yticks([])\n",
    "    ax[2][0].imshow(neighbours[1])\n",
    "    \n",
    "    ax[0][1].set_title(\"Predicted and Ground-truth difference\")\n",
    "    ax[0][1].set_xticks([])\n",
    "    ax[0][1].set_yticks([])\n",
    "    ax[0][1].imshow(cv2.absdiff(predicted, true))\n",
    "    \n",
    "    ax[1][1].set_title(\"Ground-truth frame\")\n",
    "    ax[1][1].set_xticks([])\n",
    "    ax[1][1].set_yticks([])\n",
    "    ax[1][1].imshow(true)\n",
    "    \n",
    "    ax[2][1].set_title(\"Edge shift\")\n",
    "    ax[2][1].set_xticks([])\n",
    "    ax[2][1].set_yticks([])\n",
    "    ax[2][1].imshow(predicted_marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de64e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372924b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(vis_generator, predictions, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6cfba",
   "metadata": {},
   "source": [
    "## Visualize the filters of each Conv2D layer in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a092756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_image(image, append_image, row, col, margin):\n",
    "    horizontal_start = row * height + row * margin\n",
    "    horizontal_end = horizontal_start + height\n",
    "    vertical_start = col * width + col * margin\n",
    "    vertical_end = vertical_start + width\n",
    "    image[horizontal_start : horizontal_end, vertical_start : vertical_end, : ] = append_image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196bb3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_ascent_step(model, img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        activation = model([img, img])\n",
    "        filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "        loss = tf.reduce_mean(filter_activation)\n",
    "        \n",
    "    grads = tape.gradient(loss, img)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads\n",
    "    \n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def visualize_filter(model, layer_name, filter_index, size, iterations=20, learning_rate=10.0):\n",
    "    img = tf.random.uniform((batch_size, 144, 256, 3))\n",
    "    pat_model = keras.Model(inputs=model.inputs, outputs=model.get_layer(name=layer_name).output) \n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(pat_model, img, filter_index, learning_rate)\n",
    "\n",
    "    return deprocess_image(img[0].numpy())\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def rows_cols(value):\n",
    "    assert value >= 1\n",
    "    \n",
    "    rows = 1\n",
    "    cols = value\n",
    "    for i in range(2, value//2+1):\n",
    "        if value % i == 0:\n",
    "            if np.abs(i - int(value / i)) < np.abs(rows - cols):\n",
    "                rows = i\n",
    "                cols = int(value / i)\n",
    "    \n",
    "    return rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, margin=3):\n",
    "    layers = [layer.name for layer in model.layers if type(layer) == tf.keras.layers.Conv2D]\n",
    "    layers.sort()\n",
    "    results = []\n",
    "    for layer_name in layers: \n",
    "        print(\"Layer name: \" + layer_name)\n",
    "        f_count = model.get_layer(layer_name).filters\n",
    "        rows, cols = rows_cols(f_count)\n",
    "        result = np.zeros((rows * height + (rows-1) * margin, cols * width + (cols-1) * margin, 3), dtype=np.uint8)\n",
    "        \n",
    "        for index in tqdm(range(rows*cols)):\n",
    "            i, j = index//cols, index % cols\n",
    "            filter_img = visualize_filter(model, layer_name, j + (i * cols), size=(height, width))\n",
    "            result = append_image(result, filter_img, i, j, margin)\n",
    "\n",
    "        plt.figure(figsize=(height / 2, width / 2))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c125ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filters = visualize_filters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315e4c2",
   "metadata": {},
   "source": [
    "## Visualize the predicted flow at each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_flow(u, v):\n",
    "     # calculate both the magnitude and the angle\n",
    "    magnitude, angle = cv2.cartToPolar(v, u)\n",
    "    width = u.shape[0]\n",
    "    height = u.shape[1]\n",
    "\n",
    "    # create array for the result image\n",
    "    result = np.zeros((width, height, 3), dtype='uint8')\n",
    "    result[:, :, 0] = angle * 90 / np.pi\n",
    "    result[:, :, 1] = magnitude / np.max(magnitude) * 255\n",
    "    result[:, :, 2] = 255\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_HSV2RGB);\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_flow(model, x, size=(width, height)):\n",
    "    _, _, flow_1_2, flow_2_1 = model(x)\n",
    "    flow_1_2 = np.squeeze(flow_1_2.numpy())\n",
    "    flow_2_1 = np.squeeze(flow_2_1.numpy())\n",
    "    \n",
    "    flow_1_2 = deprocess_flow(flow_1_2[:, :, 0], flow_1_2[:, :, 1])\n",
    "    flow_2_1 = deprocess_flow(flow_2_1[:, :, 0], flow_2_1[:, :, 1])\n",
    "    \n",
    "    flow_1_2 = cv2.resize(flow_1_2, size, interpolation=cv2.INTER_CUBIC)\n",
    "    flow_2_1 = cv2.resize(flow_2_1, size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return flow_1_2, flow_2_1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flows(model, x, margin=3):    \n",
    "    layers = [layer.name for layer in model.layers if type(layer) == BidirectionalFlowEstimation]\n",
    "    layers.sort()\n",
    "    \n",
    "    index = 1\n",
    "    shared_layer_index = 1\n",
    "    rows, cols = 5, 2\n",
    "    result = np.zeros((rows * height + (rows-1) * margin, cols * width + (cols-1) * margin, 3), dtype=np.uint8)\n",
    "    result = append_image(result, (x[0] * 255).astype('uint8'), 0, 0, margin)\n",
    "    result = append_image(result, (x[1] * 255).astype('uint8'), 0, 1, margin)\n",
    "    for layer_name in layers:\n",
    "        pat_model = keras.Model(inputs=model.inputs, outputs=model.get_layer(layer_name).output) \n",
    "        flow_1, flow_2 = visualize_flow(pat_model, x)\n",
    "        result = append_image(result, flow_1, index, 0, margin)\n",
    "        result = append_image(result, flow_2, index, 1, margin)\n",
    "        index += 1\n",
    "\n",
    "    plt.figure(figsize=(height / 2, width / 2))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(result)\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def visualize_flows_generator(model, generator, batch, index, margin=3):\n",
    "    neighbours = np.array(generator[batch][0])[:, index, :, :, :]\n",
    "    first = np.expand_dims(neighbours[0, :, :, :], axis=0)\n",
    "    second = np.expand_dims(neighbours[1, :, :, :], axis=0)\n",
    "    return visualize_flows(model, [first, second], margin=margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = visualize_flows_generator(model, vis_generator, 20, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321ef17",
   "metadata": {},
   "source": [
    "## Save or load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0edb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_creation_time = int(time.time())\n",
    "model.save(os.path.join(model_base_path, f'{model_name}_test.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    os.path.join(model_base_path, 'frame_booster_test.h5'),\n",
    "    custom_objects = {\n",
    "        'BidirectionalFlowEstimation': BidirectionalFlowEstimation,\n",
    "        'output_activation': output_activation,\n",
    "        'loss': loss,\n",
    "        'ssim': ssim,\n",
    "        'psnr': psnr,\n",
    "        \"l2\": l2,\n",
    "        'l1': l1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7648b",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- implement flow the same as the one used in FlowNet\n",
    "- add depth awareness to the flow prediction layer\n",
    "- decrease decoder size/change its architecture\n",
    "- add eta to fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fad236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15_2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
